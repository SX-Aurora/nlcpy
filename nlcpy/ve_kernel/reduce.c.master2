/*
#
# * The source code in this file is developed independently by NEC Corporation.
#
# # NLCPy License #
#
#     Copyright (c) 2020-2021 NEC Corporation
#     All rights reserved.
#
#     Redistribution and use in source and binary forms, with or without
#     modification, are permitted provided that the following conditions are met:
#     * Redistributions of source code must retain the above copyright notice,
#       this list of conditions and the following disclaimer.
#     * Redistributions in binary form must reproduce the above copyright notice,
#       this list of conditions and the following disclaimer in the documentation
#       and/or other materials provided with the distribution.
#     * Neither NEC Corporation nor the names of its contributors may be
#       used to endorse or promote products derived from this software
#       without specific prior written permission.
#
#     THIS SOFTWARE IS PROVIDED BY THE COPYRIGHT HOLDERS AND CONTRIBUTORS "AS IS" AND
#     ANY EXPRESS OR IMPLIED WARRANTIES, INCLUDING, BUT NOT LIMITED TO, THE IMPLIED
#     WARRANTIES OF MERCHANTABILITY AND FITNESS FOR A PARTICULAR PURPOSE ARE
#     DISCLAIMED. IN NO EVENT SHALL THE COPYRIGHT HOLDER OR CONTRIBUTORS BE LIABLE
#     FOR ANY DIRECT, INDIRECT, INCIDENTAL, SPECIAL, EXEMPLARY, OR CONSEQUENTIAL DAMAGES
#     (INCLUDING, BUT NOT LIMITED TO, PROCUREMENT OF SUBSTITUTE GOODS OR SERVICES;
#     LOSS OF USE, DATA, OR PROFITS; OR BUSINESS INTERRUPTION) HOWEVER CAUSED AND
#     ON ANY THEORY OF LIABILITY, WHETHER IN CONTRACT, STRICT LIABILITY, OR TORT
#     (INCLUDING NEGLIGENCE OR OTHERWISE) ARISING IN ANY WAY OUT OF THE USE OF THIS
#     SOFTWARE, EVEN IF ADVISED OF THE POSSIBILITY OF SUCH DAMAGE.
#
*/
@#include <stdio.h>
@#include <stdint.h>
@#include <stdbool.h>
@#include <stdlib.h>
@#include <limits.h>
@#include <alloca.h>
@#include <assert.h>
@#include <float.h>
@#include <math.h>
@#include <complex.h>

@#include "nlcpy.h"



#ifndef NLCPY_REDUCE_GLOBAL_VARIABLE
#define NLCPY_REDUCE_GLOBAL_VARIABLE
Bint nlcpy__global_bool;
uint32_t nlcpy__global_u32;
uint64_t nlcpy__global_u64;
int32_t nlcpy__global_i32;
int64_t nlcpy__global_i64;
float nlcpy__global_f32;
double nlcpy__global_f64;
float _Complex nlcpy__global_c64;
double _Complex nlcpy__global_c128;
#endif

/****************************
 *
 *       REDUCE OPERATOR
 *
 * **************************/

#if defined(DTAG_i32)

uint64_t FILENAME_@DTAG1@_i32(ve_array *x, ve_array *y, int32_t axis, int32_t init_flag,
                     ve_array *initial, int32_t where_flag, ve_array *where, int32_t *psw)
{
    @TYPE1@ *px = (@TYPE1@ *)nlcpy__get_ptr(x);
    if (px == NULL) return NLCPY_ERROR_MEMORY;
    int32_t *py = (int32_t *)nlcpy__get_ptr(y);
    if (py == NULL) return NLCPY_ERROR_MEMORY;
    int32_t *pi = (int32_t *)nlcpy__get_ptr(initial);
    if (pi == NULL) return NLCPY_ERROR_MEMORY;
    ve_array *w = where;
    int32_t init_val = (int32_t)(*pi);

    // initialize
    if (init_flag) {
@#ifdef _OPENMP
@#pragma omp for
@#endif /* _OPENMP */
        for (uint64_t i=0; i<y->size; i++) py[i] = init_val;
    }
    else {
@#ifdef _OPENMP
@#pragma omp for
@#endif /* _OPENMP */
        for (uint64_t i=0; i<y->size; i++) py[i] = 0;
    }

/////////
// 0-d //
/////////
    if (x->ndim == 0){
@#ifdef _OPENMP
@#pragma omp single
@#endif /* _OPENMP */
{
        if (!where_flag) {
            if (!init_flag) {
                *py = *px;
            }
            else{
                @UNARY_OPERATOR@(*px,*py,i32)
            }
        } else {
            Bint *pw = (Bint *)w->ve_adr;
            if (*pw) {
                if (!init_flag) {
                    *py = *px;
                }
                else{
                    @UNARY_OPERATOR@(*px,*py,i32)
                }
            }
        }
} /* omp single */

/////////
// 1-d //
/////////
    } else if (x->ndim == 1){
        uint64_t i;
        uint64_t iw0 = 0;
        const uint64_t ix = x->strides[0]/x->itemsize;
        if (!where_flag) {
#if defined(add_reduce) || defined(multiply_reduce)
            int32_t tmp;
#if defined(add_reduce)
            nlcpy__global_i32 = 0;
@#pragma omp barrier
            tmp = 0;
#elif defined(multiply_reduce)
            nlcpy__global_i32 = 1;
@#pragma omp barrier
            tmp = 1;
#else
#error add_reduce or minimum_reduce must be defined.
#endif
            uint64_t ii;
            if (!init_flag) {
                *py = *px;
                ii = 1;
            } else {
                ii = 0;
            }
            const int it = omp_get_thread_num();
            const int nt = omp_get_max_threads();
            const uint64_t ist = (x->shape[0]-ii)*it/nt;
            const uint64_t ien = (x->shape[0]-ii)*(it+1)/nt;
            if (it==0) tmp = (int32_t)(*py);
@#pragma _NEC ivdep
            for (i=ii+ist; i < ii+ien; i++) {
                @UNARY_OPERATOR@(px[i*ix],tmp,i32)
            }
@#pragma _NEC novector
            for (i=0; i < nt; i++) {
                if (i==it) {
                    @UNARY_OPERATOR@(tmp,nlcpy__global_i32,i32)
                }
@#pragma omp barrier
            }
            *py = nlcpy__global_i32;
#elif ( defined(maximum_reduce) || defined(minimum_reduce) ) && ( defined(DTAG1_i32) || defined(DTAG1_i64) || defined(DTAG1_f32) || defined(DTAG1_f64) || defined(DTAG1_bool)) && ( "i32" eq "i32" || "i32" eq "i64" || "i32" eq "f32" || "i32" eq "f64" || "i32" eq "bool")
            // Note that rational operations for complex and unsigned numbers occurs an error or warnning.
            uint64_t ii;
            if (!init_flag) {
                *py = *px;
                ii = 1;
            } else {
                ii = 0;
            }
            nlcpy__global_i32 = (int32_t)(*py);
@#pragma omp barrier
            const int it = omp_get_thread_num();
            const int nt = omp_get_max_threads();
            const uint64_t ist = (x->shape[0]-ii)*it/nt;
            const uint64_t ien = (x->shape[0]-ii)*(it+1)/nt;
            int32_t tmp = (int32_t)(*py);
#if "i32" eq "f32" || "i32" eq "f64"
            double is_there_nan = (isnan_i32(tmp)) ? 1.0 : 0.0;
#elif "i32" eq "i32" || "i32" eq "i64" || "i32" eq "bool"
            double is_there_nan = 0.0;
#else
#error Not Impletended
#endif
@#pragma _NEC ivdep
            for (i=ii+ist; i < ii+ien; i++) {
#if defined(maximum_reduce)
                tmp = (tmp > px[i*ix]) ? tmp : (int32_t)px[i*ix];
#elif defined(minimum_reduce)
                tmp = (tmp < px[i*ix]) ? tmp : (int32_t)px[i*ix];
#endif
#if defined(DTAG1_f32) || defined(DTAG1_f64)
                // NaN check:
                // is_there_nan += (float)isnan_@DTAG1@(px[i*ix]);
                @TYPE1@ xx;
                *(&xx) = px[i*ix];
                // The following line checks for NaN and it is in an unnatural way.
                // Here, NaN == NaN becomes False.
                // If there is one or more qNaN, signaling NaN (sNaN) occurs.
                // qNaN might not be detected qNaN by compiler optimizations.
                // However, we prioritize the performance.
                is_there_nan += (! (xx == px[i*ix]) ) ? 1.0 : 0.0;
#endif
            }
#if defined(DTAG1_f32) || defined(DTAG1_f64)
            if (is_there_nan != (float)0) {
                tmp = NAN;
                // In the following function call, PSW flags are manipulated to skip the intentional sNaN above.
                retrieve_fpe_flags(psw);
                *psw &= 0x0000003d;
                set_fpe_flags(*psw);
            }
#endif
@#pragma omp critical
            {
                @UNARY_OPERATOR@(tmp,nlcpy__global_i32,i32)
            }
@#pragma omp barrier
            *py = nlcpy__global_i32;
#else
        // Unvectorizable case
@#ifdef _OPENMP
@#pragma omp single
@#endif /* _OPENMP */
            {
                if (!init_flag) {
                    *py = *px;
                    i = 1;
                } else {
                    i = 0;
                }
@#pragma _NEC novector
                for (; i < x->shape[0]; i++) {
                    @UNARY_OPERATOR@(px[i*ix],*py,i32)
                }
            } /* omp single */
#endif
        } else {
@#ifdef _OPENMP
@#pragma omp single
@#endif /* _OPENMP */
            {
                Bint *pw = (Bint *)w->ve_adr;
                const uint64_t iw = w->strides[0]/w->itemsize;
                if (!init_flag) {
                    for (i = 0; i < x->shape[0]; i++) {
                        if (pw[i*iw]) {
                            *py = *px;
                            i++;
                            break;
                        }
                    }
                } else {
                    i = 0;
                }
#ifdef left_shift_reduce
@#pragma _NEC novector
#else
@#pragma _NEC ivdep
#endif
                for (; i < x->shape[0]; i++) {
                    if (pw[i*iw]) {
                        @UNARY_OPERATOR@(px[i*ix],*py,i32)
                    }
                }
            } /* omp single */
        }

/////////
// N-d //
/////////
    } else if (x->ndim <= NLCPY_MAXNDIM) {
        int64_t n_inner = x->ndim - 1;
        int64_t n_outer = 0;

        int64_t *idx = (int64_t*)alloca(sizeof(int64_t)*x->ndim);
        for (uint64_t i = 0; i < x->ndim; i++) {
            idx[i] = i;
        }
        if (x->ndim > 2) {
            int64_t max_idx[3], tmp;
            nlcpy__argnsort(x, max_idx, 3);
#ifndef add_reduce
            if (idx[max_idx[0]] == axis) {
                max_idx[0] = max_idx[1];
            }
#endif
            if (max_idx[0] != n_inner) {
                tmp = idx[n_inner];
                idx[n_inner] = idx[max_idx[0]];
                idx[max_idx[0]] = tmp;
            }
            if (idx[max_idx[1]] == axis) {
                max_idx[1] = max_idx[2];
            }
            if (idx[max_idx[1]] != n_outer) {
                tmp = idx[n_outer];
                idx[n_outer] = idx[max_idx[1]];
                idx[max_idx[1]] = tmp;
            }
        } else if (n_outer == axis) {
            idx[0] = 1;
            idx[1] = 0;
        }
        int64_t n_inner2 = idx[n_inner];
        int64_t n_outer2 = idx[n_outer];

@#ifdef _OPENMP
        const int nt = omp_get_num_threads();
        const int it = omp_get_thread_num();
@#else
        const int nt = 1;
        const int it = 0;
@#endif
        int64_t i;
        uint64_t adr_x = (uint64_t)x->ve_adr;
        uint64_t adr_y = (uint64_t)y->ve_adr;

        uint64_t ix = 0;
        uint64_t iy = 0;
        uint64_t iw = 0;
        uint64_t ix0 = x->strides[n_inner2] / x->itemsize;
        uint64_t iy0 = y->strides[n_inner2] / y->itemsize;
        uint64_t iw0 = 0;
        int64_t k = 0;
        int64_t *cnt_x = (int64_t*)alloca(sizeof(int64_t)*x->ndim);
        nlcpy__reset_coords(cnt_x, x->ndim);
        const int64_t lenm = x->shape[n_outer2];
        const int64_t cntm_s = lenm * it / nt;
        const int64_t cntm_e = lenm * (it + 1) / nt;
        if (!where_flag) {
            for (int64_t cntm = cntm_s; cntm < cntm_e; cntm++) {
                cnt_x[n_outer2] = cntm;
                ix = cntm * x->strides[n_outer2] / x->itemsize;
                iy = (n_outer2 == axis) ? 0 : cntm * y->strides[n_outer2] / y->itemsize;

                do {
                    if (n_inner2 == axis) {
                        // Note that a rational operation for complex and unsigned numbers occurs an error or warnning.
                        uint64_t ii;
                        if (!init_flag) {
                           py[iy] = px[ix];
                           ii = 1;
                        } else {
                           ii = 0;
                        }
#if ( defined(maximum_reduce) || defined(minimum_reduce) ) && ( defined(DTAG1_i32) || defined(DTAG1_i64) || defined(DTAG1_f32) || defined(DTAG1_f64) || defined(DTAG1_bool)) && ( "i32" eq "i32" || "i32" eq "i64" || "i32" eq "f32" || "i32" eq "f64" || "i32" eq "bool")
                        int32_t tmp = py[iy];
#if "i32" eq "f32" || "i32" eq "f64"
                        float is_there_nan = (isnan_i32(py[iy])) ? 1.0 : 0.0;
#elif "i32" eq "i32" || "i32" eq "i64" || "i32" eq "bool"
                        float is_there_nan = 0.0;
#else
#error Not Impletended
#endif
@#pragma _NEC ivdep
                        for (i = ii; i < x->shape[n_inner2]; i++) {
#if defined(maximum_reduce)
                            tmp = (tmp > px[i*ix0+ix]) ? tmp : (int32_t)px[i*ix0+ix];
#elif defined(minimum_reduce)
                            tmp = (tmp < px[i*ix0+ix]) ? tmp : (int32_t)px[i*ix0+ix];
#endif
#if defined(DTAG1_f32) || defined(DTAG1_f64)
                            // NaN check:
                            //   is_there_nan += (float)isnan_@DTAG1@(px[i*ix0+ix]);
                            @TYPE1@ xx;
                            *(&xx) = px[i*ix0+ix];
                            // The following line checks for NaN and it is in an unnatural way.
                            // Here, NaN == NaN becomes False.
                            // If there is one or more qNaN, signaling NaN (sNaN) occurs.
                            // qNaN might not be detected qNaN by compiler optimizations.
                            // However, we prioritize the performance.
                            is_there_nan += (! (xx == px[i*ix0+ix]) ) ? 1.0 : 0.0;
#endif
                        }
#if defined(DTAG1_f32) || defined(DTAG1_f64)
                        if (is_there_nan != (float)0) {
                            tmp = NAN;
                            // In the following function call, PSW flags are manipulated to skip the intentional sNaN above.
                            retrieve_fpe_flags(psw);
                            *psw &= 0x0000003d;
                            set_fpe_flags(*psw);
                        }
#endif
                        py[iy] = tmp;
#else
                        // General case
#ifdef left_shift_reduce
@#pragma _NEC novector
#else
@#pragma _NEC ivdep
#endif
                        for (i = ii; i < x->shape[n_inner2]; i++) {
                            @UNARY_OPERATOR@(px[i*ix0+ix],py[iy],i32)
                        }
#endif
                    } else {
                        if (init_flag || cnt_x[axis] > 0) {
#ifdef left_shift_reduce
@#pragma _NEC novector
#else
@#pragma _NEC ivdep
#endif
                            for (i = 0; i < x->shape[n_inner2]; i++) {
                                @UNARY_OPERATOR@(px[i*ix0+ix],py[i*iy0+iy],i32)
                             }
                        } else {
                            for (i = 0; i < x->shape[n_inner2]; i++) {
                                py[i*iy0+iy] = @CAST_OPERATOR@(px[i*ix0+ix],@DTAG1@,i32)
                            }
                        }
                    }
                    for (k = n_inner - 1; k >= 1; k--) {
                        int64_t kk = idx[k];
                        if (kk == axis) {
                            if (++cnt_x[kk] < x->shape[kk]) {
                                ix += x->strides[kk] / x->itemsize;
                                break;
                            }
                            ix -= (x->strides[kk] / x->itemsize) * (x->shape[kk] - 1);
                        } else {
                            if (++cnt_x[kk] < x->shape[kk]) {
                                ix += x->strides[kk] / x->itemsize;
                                iy += y->strides[kk] / y->itemsize;
                                break;
                            }
                            ix -= (x->strides[kk] / x->itemsize) * (x->shape[kk] - 1);
                            iy -= (y->strides[kk] / y->itemsize) * (y->shape[kk] - 1);
                        }
                        cnt_x[kk] = 0;
                    }
                } while (k >= 1);
            }
        } else {
            Bint *pw = (Bint *)w->ve_adr;
            iw0 = w->strides[n_inner2] / w->itemsize;

            for (int64_t cntm = cntm_s; cntm < cntm_e; cntm++) {
                ix = cntm * x->strides[n_outer2] / x->itemsize;
                iy = (n_outer2 == axis) ? 0 : cntm * y->strides[n_outer2] / y->itemsize;
                iw = cntm * w->strides[n_outer2] / w->itemsize;

                do {
                    if (n_inner2 == axis) {
                        if (!init_flag) {
                            for (i = 0; i < w->shape[n_inner2]; i++) {
                                if (pw[i*iw0+iw]) {
                                    py[iy] = @CAST_OPERATOR@(px[i*ix0+ix],@DTAG1@,i32)
                                    i++;
                                    break;
                                }
                            }
                        } else {
                            i = 0;
                        }
#ifdef left_shift_reduce
@#pragma _NEC novector
#else
@#pragma _NEC ivdep
#endif
                        for (; i < x->shape[n_inner2]; i++) {
                            if (pw[i*iw0+iw]) {
                                @UNARY_OPERATOR@(px[i*ix0+ix],py[iy],i32)
                            }
                        }
                    } else {
                        if (init_flag || cnt_x[axis] > 0) {
#ifdef left_shift_reduce
@#pragma _NEC novector
#else
@#pragma _NEC ivdep
#endif
                            for (i = 0; i < x->shape[n_inner2]; i++) {
                                if (pw[i*iw0+iw]) {
                                    @UNARY_OPERATOR@(px[i*ix0+ix],py[i*iy0+iy],i32)
                                }
                            }
                        } else {
                            for (i = 0; i < x->shape[n_inner2]; i++) {
                                if (pw[i*iw0+iw]) {
                                    py[i*iy0+iy] = @CAST_OPERATOR@(px[i*ix0+ix],@DTAG1@,i32)
                                }
                            }
                        }
                    }
                    for (k = n_inner - 1; k >= 1; k--) {
                        int64_t kk = idx[k];
                        if (kk == axis) {
                            if (++cnt_x[kk] < x->shape[kk]) {
                                ix += x->strides[kk] / x->itemsize;
                                iw += w->strides[kk] / w->itemsize;
                                break;
                            }
                            ix -= (x->strides[kk] / x->itemsize) * (x->shape[kk] - 1);
                            iw -= (w->strides[kk] / w->itemsize) * (x->shape[kk] - 1);
                        } else {
                            if (++cnt_x[kk] < x->shape[kk]) {
                                ix += x->strides[kk] / x->itemsize;
                                iy += y->strides[kk] / y->itemsize;
                                iw += w->strides[kk] / w->itemsize;
                                break;
                            }
                            ix -= (x->strides[kk] / x->itemsize) * (x->shape[kk] - 1);
                            iy -= (y->strides[kk] / y->itemsize) * (x->shape[kk] - 1);
                            iw -= (w->strides[kk] / w->itemsize) * (x->shape[kk] - 1);
                        }
                        cnt_x[kk] = 0;
                    }
                } while (k >= 1);
            }
        }
    } else {
        return (uint64_t)NLCPY_ERROR_NDIM;
    }
    retrieve_fpe_flags(psw);
    return (uint64_t)NLCPY_ERROR_OK;
}


#endif
#if defined(DTAG_i64)

uint64_t FILENAME_@DTAG1@_i64(ve_array *x, ve_array *y, int32_t axis, int32_t init_flag,
                     ve_array *initial, int32_t where_flag, ve_array *where, int32_t *psw)
{
    @TYPE1@ *px = (@TYPE1@ *)nlcpy__get_ptr(x);
    if (px == NULL) return NLCPY_ERROR_MEMORY;
    int64_t *py = (int64_t *)nlcpy__get_ptr(y);
    if (py == NULL) return NLCPY_ERROR_MEMORY;
    int64_t *pi = (int64_t *)nlcpy__get_ptr(initial);
    if (pi == NULL) return NLCPY_ERROR_MEMORY;
    ve_array *w = where;
    int64_t init_val = (int64_t)(*pi);

    // initialize
    if (init_flag) {
@#ifdef _OPENMP
@#pragma omp for
@#endif /* _OPENMP */
        for (uint64_t i=0; i<y->size; i++) py[i] = init_val;
    }
    else {
@#ifdef _OPENMP
@#pragma omp for
@#endif /* _OPENMP */
        for (uint64_t i=0; i<y->size; i++) py[i] = 0;
    }

/////////
// 0-d //
/////////
    if (x->ndim == 0){
@#ifdef _OPENMP
@#pragma omp single
@#endif /* _OPENMP */
{
        if (!where_flag) {
            if (!init_flag) {
                *py = *px;
            }
            else{
                @UNARY_OPERATOR@(*px,*py,i64)
            }
        } else {
            Bint *pw = (Bint *)w->ve_adr;
            if (*pw) {
                if (!init_flag) {
                    *py = *px;
                }
                else{
                    @UNARY_OPERATOR@(*px,*py,i64)
                }
            }
        }
} /* omp single */

/////////
// 1-d //
/////////
    } else if (x->ndim == 1){
        uint64_t i;
        uint64_t iw0 = 0;
        const uint64_t ix = x->strides[0]/x->itemsize;
        if (!where_flag) {
#if defined(add_reduce) || defined(multiply_reduce)
            int64_t tmp;
#if defined(add_reduce)
            nlcpy__global_i64 = 0;
@#pragma omp barrier
            tmp = 0;
#elif defined(multiply_reduce)
            nlcpy__global_i64 = 1;
@#pragma omp barrier
            tmp = 1;
#else
#error add_reduce or minimum_reduce must be defined.
#endif
            uint64_t ii;
            if (!init_flag) {
                *py = *px;
                ii = 1;
            } else {
                ii = 0;
            }
            const int it = omp_get_thread_num();
            const int nt = omp_get_max_threads();
            const uint64_t ist = (x->shape[0]-ii)*it/nt;
            const uint64_t ien = (x->shape[0]-ii)*(it+1)/nt;
            if (it==0) tmp = (int64_t)(*py);
@#pragma _NEC ivdep
            for (i=ii+ist; i < ii+ien; i++) {
                @UNARY_OPERATOR@(px[i*ix],tmp,i64)
            }
@#pragma _NEC novector
            for (i=0; i < nt; i++) {
                if (i==it) {
                    @UNARY_OPERATOR@(tmp,nlcpy__global_i64,i64)
                }
@#pragma omp barrier
            }
            *py = nlcpy__global_i64;
#elif ( defined(maximum_reduce) || defined(minimum_reduce) ) && ( defined(DTAG1_i32) || defined(DTAG1_i64) || defined(DTAG1_f32) || defined(DTAG1_f64) || defined(DTAG1_bool)) && ( "i64" eq "i32" || "i64" eq "i64" || "i64" eq "f32" || "i64" eq "f64" || "i64" eq "bool")
            // Note that rational operations for complex and unsigned numbers occurs an error or warnning.
            uint64_t ii;
            if (!init_flag) {
                *py = *px;
                ii = 1;
            } else {
                ii = 0;
            }
            nlcpy__global_i64 = (int64_t)(*py);
@#pragma omp barrier
            const int it = omp_get_thread_num();
            const int nt = omp_get_max_threads();
            const uint64_t ist = (x->shape[0]-ii)*it/nt;
            const uint64_t ien = (x->shape[0]-ii)*(it+1)/nt;
            int64_t tmp = (int64_t)(*py);
#if "i64" eq "f32" || "i64" eq "f64"
            double is_there_nan = (isnan_i64(tmp)) ? 1.0 : 0.0;
#elif "i64" eq "i32" || "i64" eq "i64" || "i64" eq "bool"
            double is_there_nan = 0.0;
#else
#error Not Impletended
#endif
@#pragma _NEC ivdep
            for (i=ii+ist; i < ii+ien; i++) {
#if defined(maximum_reduce)
                tmp = (tmp > px[i*ix]) ? tmp : (int64_t)px[i*ix];
#elif defined(minimum_reduce)
                tmp = (tmp < px[i*ix]) ? tmp : (int64_t)px[i*ix];
#endif
#if defined(DTAG1_f32) || defined(DTAG1_f64)
                // NaN check:
                // is_there_nan += (float)isnan_@DTAG1@(px[i*ix]);
                @TYPE1@ xx;
                *(&xx) = px[i*ix];
                // The following line checks for NaN and it is in an unnatural way.
                // Here, NaN == NaN becomes False.
                // If there is one or more qNaN, signaling NaN (sNaN) occurs.
                // qNaN might not be detected qNaN by compiler optimizations.
                // However, we prioritize the performance.
                is_there_nan += (! (xx == px[i*ix]) ) ? 1.0 : 0.0;
#endif
            }
#if defined(DTAG1_f32) || defined(DTAG1_f64)
            if (is_there_nan != (float)0) {
                tmp = NAN;
                // In the following function call, PSW flags are manipulated to skip the intentional sNaN above.
                retrieve_fpe_flags(psw);
                *psw &= 0x0000003d;
                set_fpe_flags(*psw);
            }
#endif
@#pragma omp critical
            {
                @UNARY_OPERATOR@(tmp,nlcpy__global_i64,i64)
            }
@#pragma omp barrier
            *py = nlcpy__global_i64;
#else
        // Unvectorizable case
@#ifdef _OPENMP
@#pragma omp single
@#endif /* _OPENMP */
            {
                if (!init_flag) {
                    *py = *px;
                    i = 1;
                } else {
                    i = 0;
                }
@#pragma _NEC novector
                for (; i < x->shape[0]; i++) {
                    @UNARY_OPERATOR@(px[i*ix],*py,i64)
                }
            } /* omp single */
#endif
        } else {
@#ifdef _OPENMP
@#pragma omp single
@#endif /* _OPENMP */
            {
                Bint *pw = (Bint *)w->ve_adr;
                const uint64_t iw = w->strides[0]/w->itemsize;
                if (!init_flag) {
                    for (i = 0; i < x->shape[0]; i++) {
                        if (pw[i*iw]) {
                            *py = *px;
                            i++;
                            break;
                        }
                    }
                } else {
                    i = 0;
                }
#ifdef left_shift_reduce
@#pragma _NEC novector
#else
@#pragma _NEC ivdep
#endif
                for (; i < x->shape[0]; i++) {
                    if (pw[i*iw]) {
                        @UNARY_OPERATOR@(px[i*ix],*py,i64)
                    }
                }
            } /* omp single */
        }

/////////
// N-d //
/////////
    } else if (x->ndim <= NLCPY_MAXNDIM) {
        int64_t n_inner = x->ndim - 1;
        int64_t n_outer = 0;

        int64_t *idx = (int64_t*)alloca(sizeof(int64_t)*x->ndim);
        for (uint64_t i = 0; i < x->ndim; i++) {
            idx[i] = i;
        }
        if (x->ndim > 2) {
            int64_t max_idx[3], tmp;
            nlcpy__argnsort(x, max_idx, 3);
#ifndef add_reduce
            if (idx[max_idx[0]] == axis) {
                max_idx[0] = max_idx[1];
            }
#endif
            if (max_idx[0] != n_inner) {
                tmp = idx[n_inner];
                idx[n_inner] = idx[max_idx[0]];
                idx[max_idx[0]] = tmp;
            }
            if (idx[max_idx[1]] == axis) {
                max_idx[1] = max_idx[2];
            }
            if (idx[max_idx[1]] != n_outer) {
                tmp = idx[n_outer];
                idx[n_outer] = idx[max_idx[1]];
                idx[max_idx[1]] = tmp;
            }
        } else if (n_outer == axis) {
            idx[0] = 1;
            idx[1] = 0;
        }
        int64_t n_inner2 = idx[n_inner];
        int64_t n_outer2 = idx[n_outer];

@#ifdef _OPENMP
        const int nt = omp_get_num_threads();
        const int it = omp_get_thread_num();
@#else
        const int nt = 1;
        const int it = 0;
@#endif
        int64_t i;
        uint64_t adr_x = (uint64_t)x->ve_adr;
        uint64_t adr_y = (uint64_t)y->ve_adr;

        uint64_t ix = 0;
        uint64_t iy = 0;
        uint64_t iw = 0;
        uint64_t ix0 = x->strides[n_inner2] / x->itemsize;
        uint64_t iy0 = y->strides[n_inner2] / y->itemsize;
        uint64_t iw0 = 0;
        int64_t k = 0;
        int64_t *cnt_x = (int64_t*)alloca(sizeof(int64_t)*x->ndim);
        nlcpy__reset_coords(cnt_x, x->ndim);
        const int64_t lenm = x->shape[n_outer2];
        const int64_t cntm_s = lenm * it / nt;
        const int64_t cntm_e = lenm * (it + 1) / nt;
        if (!where_flag) {
            for (int64_t cntm = cntm_s; cntm < cntm_e; cntm++) {
                cnt_x[n_outer2] = cntm;
                ix = cntm * x->strides[n_outer2] / x->itemsize;
                iy = (n_outer2 == axis) ? 0 : cntm * y->strides[n_outer2] / y->itemsize;

                do {
                    if (n_inner2 == axis) {
                        // Note that a rational operation for complex and unsigned numbers occurs an error or warnning.
                        uint64_t ii;
                        if (!init_flag) {
                           py[iy] = px[ix];
                           ii = 1;
                        } else {
                           ii = 0;
                        }
#if ( defined(maximum_reduce) || defined(minimum_reduce) ) && ( defined(DTAG1_i32) || defined(DTAG1_i64) || defined(DTAG1_f32) || defined(DTAG1_f64) || defined(DTAG1_bool)) && ( "i64" eq "i32" || "i64" eq "i64" || "i64" eq "f32" || "i64" eq "f64" || "i64" eq "bool")
                        int64_t tmp = py[iy];
#if "i64" eq "f32" || "i64" eq "f64"
                        float is_there_nan = (isnan_i64(py[iy])) ? 1.0 : 0.0;
#elif "i64" eq "i32" || "i64" eq "i64" || "i64" eq "bool"
                        float is_there_nan = 0.0;
#else
#error Not Impletended
#endif
@#pragma _NEC ivdep
                        for (i = ii; i < x->shape[n_inner2]; i++) {
#if defined(maximum_reduce)
                            tmp = (tmp > px[i*ix0+ix]) ? tmp : (int64_t)px[i*ix0+ix];
#elif defined(minimum_reduce)
                            tmp = (tmp < px[i*ix0+ix]) ? tmp : (int64_t)px[i*ix0+ix];
#endif
#if defined(DTAG1_f32) || defined(DTAG1_f64)
                            // NaN check:
                            //   is_there_nan += (float)isnan_@DTAG1@(px[i*ix0+ix]);
                            @TYPE1@ xx;
                            *(&xx) = px[i*ix0+ix];
                            // The following line checks for NaN and it is in an unnatural way.
                            // Here, NaN == NaN becomes False.
                            // If there is one or more qNaN, signaling NaN (sNaN) occurs.
                            // qNaN might not be detected qNaN by compiler optimizations.
                            // However, we prioritize the performance.
                            is_there_nan += (! (xx == px[i*ix0+ix]) ) ? 1.0 : 0.0;
#endif
                        }
#if defined(DTAG1_f32) || defined(DTAG1_f64)
                        if (is_there_nan != (float)0) {
                            tmp = NAN;
                            // In the following function call, PSW flags are manipulated to skip the intentional sNaN above.
                            retrieve_fpe_flags(psw);
                            *psw &= 0x0000003d;
                            set_fpe_flags(*psw);
                        }
#endif
                        py[iy] = tmp;
#else
                        // General case
#ifdef left_shift_reduce
@#pragma _NEC novector
#else
@#pragma _NEC ivdep
#endif
                        for (i = ii; i < x->shape[n_inner2]; i++) {
                            @UNARY_OPERATOR@(px[i*ix0+ix],py[iy],i64)
                        }
#endif
                    } else {
                        if (init_flag || cnt_x[axis] > 0) {
#ifdef left_shift_reduce
@#pragma _NEC novector
#else
@#pragma _NEC ivdep
#endif
                            for (i = 0; i < x->shape[n_inner2]; i++) {
                                @UNARY_OPERATOR@(px[i*ix0+ix],py[i*iy0+iy],i64)
                             }
                        } else {
                            for (i = 0; i < x->shape[n_inner2]; i++) {
                                py[i*iy0+iy] = @CAST_OPERATOR@(px[i*ix0+ix],@DTAG1@,i64)
                            }
                        }
                    }
                    for (k = n_inner - 1; k >= 1; k--) {
                        int64_t kk = idx[k];
                        if (kk == axis) {
                            if (++cnt_x[kk] < x->shape[kk]) {
                                ix += x->strides[kk] / x->itemsize;
                                break;
                            }
                            ix -= (x->strides[kk] / x->itemsize) * (x->shape[kk] - 1);
                        } else {
                            if (++cnt_x[kk] < x->shape[kk]) {
                                ix += x->strides[kk] / x->itemsize;
                                iy += y->strides[kk] / y->itemsize;
                                break;
                            }
                            ix -= (x->strides[kk] / x->itemsize) * (x->shape[kk] - 1);
                            iy -= (y->strides[kk] / y->itemsize) * (y->shape[kk] - 1);
                        }
                        cnt_x[kk] = 0;
                    }
                } while (k >= 1);
            }
        } else {
            Bint *pw = (Bint *)w->ve_adr;
            iw0 = w->strides[n_inner2] / w->itemsize;

            for (int64_t cntm = cntm_s; cntm < cntm_e; cntm++) {
                ix = cntm * x->strides[n_outer2] / x->itemsize;
                iy = (n_outer2 == axis) ? 0 : cntm * y->strides[n_outer2] / y->itemsize;
                iw = cntm * w->strides[n_outer2] / w->itemsize;

                do {
                    if (n_inner2 == axis) {
                        if (!init_flag) {
                            for (i = 0; i < w->shape[n_inner2]; i++) {
                                if (pw[i*iw0+iw]) {
                                    py[iy] = @CAST_OPERATOR@(px[i*ix0+ix],@DTAG1@,i64)
                                    i++;
                                    break;
                                }
                            }
                        } else {
                            i = 0;
                        }
#ifdef left_shift_reduce
@#pragma _NEC novector
#else
@#pragma _NEC ivdep
#endif
                        for (; i < x->shape[n_inner2]; i++) {
                            if (pw[i*iw0+iw]) {
                                @UNARY_OPERATOR@(px[i*ix0+ix],py[iy],i64)
                            }
                        }
                    } else {
                        if (init_flag || cnt_x[axis] > 0) {
#ifdef left_shift_reduce
@#pragma _NEC novector
#else
@#pragma _NEC ivdep
#endif
                            for (i = 0; i < x->shape[n_inner2]; i++) {
                                if (pw[i*iw0+iw]) {
                                    @UNARY_OPERATOR@(px[i*ix0+ix],py[i*iy0+iy],i64)
                                }
                            }
                        } else {
                            for (i = 0; i < x->shape[n_inner2]; i++) {
                                if (pw[i*iw0+iw]) {
                                    py[i*iy0+iy] = @CAST_OPERATOR@(px[i*ix0+ix],@DTAG1@,i64)
                                }
                            }
                        }
                    }
                    for (k = n_inner - 1; k >= 1; k--) {
                        int64_t kk = idx[k];
                        if (kk == axis) {
                            if (++cnt_x[kk] < x->shape[kk]) {
                                ix += x->strides[kk] / x->itemsize;
                                iw += w->strides[kk] / w->itemsize;
                                break;
                            }
                            ix -= (x->strides[kk] / x->itemsize) * (x->shape[kk] - 1);
                            iw -= (w->strides[kk] / w->itemsize) * (x->shape[kk] - 1);
                        } else {
                            if (++cnt_x[kk] < x->shape[kk]) {
                                ix += x->strides[kk] / x->itemsize;
                                iy += y->strides[kk] / y->itemsize;
                                iw += w->strides[kk] / w->itemsize;
                                break;
                            }
                            ix -= (x->strides[kk] / x->itemsize) * (x->shape[kk] - 1);
                            iy -= (y->strides[kk] / y->itemsize) * (x->shape[kk] - 1);
                            iw -= (w->strides[kk] / w->itemsize) * (x->shape[kk] - 1);
                        }
                        cnt_x[kk] = 0;
                    }
                } while (k >= 1);
            }
        }
    } else {
        return (uint64_t)NLCPY_ERROR_NDIM;
    }
    retrieve_fpe_flags(psw);
    return (uint64_t)NLCPY_ERROR_OK;
}


#endif
#if defined(DTAG_u32)

uint64_t FILENAME_@DTAG1@_u32(ve_array *x, ve_array *y, int32_t axis, int32_t init_flag,
                     ve_array *initial, int32_t where_flag, ve_array *where, int32_t *psw)
{
    @TYPE1@ *px = (@TYPE1@ *)nlcpy__get_ptr(x);
    if (px == NULL) return NLCPY_ERROR_MEMORY;
    uint32_t *py = (uint32_t *)nlcpy__get_ptr(y);
    if (py == NULL) return NLCPY_ERROR_MEMORY;
    uint32_t *pi = (uint32_t *)nlcpy__get_ptr(initial);
    if (pi == NULL) return NLCPY_ERROR_MEMORY;
    ve_array *w = where;
    uint32_t init_val = (uint32_t)(*pi);

    // initialize
    if (init_flag) {
@#ifdef _OPENMP
@#pragma omp for
@#endif /* _OPENMP */
        for (uint64_t i=0; i<y->size; i++) py[i] = init_val;
    }
    else {
@#ifdef _OPENMP
@#pragma omp for
@#endif /* _OPENMP */
        for (uint64_t i=0; i<y->size; i++) py[i] = 0;
    }

/////////
// 0-d //
/////////
    if (x->ndim == 0){
@#ifdef _OPENMP
@#pragma omp single
@#endif /* _OPENMP */
{
        if (!where_flag) {
            if (!init_flag) {
                *py = *px;
            }
            else{
                @UNARY_OPERATOR@(*px,*py,u32)
            }
        } else {
            Bint *pw = (Bint *)w->ve_adr;
            if (*pw) {
                if (!init_flag) {
                    *py = *px;
                }
                else{
                    @UNARY_OPERATOR@(*px,*py,u32)
                }
            }
        }
} /* omp single */

/////////
// 1-d //
/////////
    } else if (x->ndim == 1){
        uint64_t i;
        uint64_t iw0 = 0;
        const uint64_t ix = x->strides[0]/x->itemsize;
        if (!where_flag) {
#if defined(add_reduce) || defined(multiply_reduce)
            uint32_t tmp;
#if defined(add_reduce)
            nlcpy__global_u32 = 0;
@#pragma omp barrier
            tmp = 0;
#elif defined(multiply_reduce)
            nlcpy__global_u32 = 1;
@#pragma omp barrier
            tmp = 1;
#else
#error add_reduce or minimum_reduce must be defined.
#endif
            uint64_t ii;
            if (!init_flag) {
                *py = *px;
                ii = 1;
            } else {
                ii = 0;
            }
            const int it = omp_get_thread_num();
            const int nt = omp_get_max_threads();
            const uint64_t ist = (x->shape[0]-ii)*it/nt;
            const uint64_t ien = (x->shape[0]-ii)*(it+1)/nt;
            if (it==0) tmp = (uint32_t)(*py);
@#pragma _NEC ivdep
            for (i=ii+ist; i < ii+ien; i++) {
                @UNARY_OPERATOR@(px[i*ix],tmp,u32)
            }
@#pragma _NEC novector
            for (i=0; i < nt; i++) {
                if (i==it) {
                    @UNARY_OPERATOR@(tmp,nlcpy__global_u32,u32)
                }
@#pragma omp barrier
            }
            *py = nlcpy__global_u32;
#elif ( defined(maximum_reduce) || defined(minimum_reduce) ) && ( defined(DTAG1_i32) || defined(DTAG1_i64) || defined(DTAG1_f32) || defined(DTAG1_f64) || defined(DTAG1_bool)) && ( "u32" eq "i32" || "u32" eq "i64" || "u32" eq "f32" || "u32" eq "f64" || "u32" eq "bool")
            // Note that rational operations for complex and unsigned numbers occurs an error or warnning.
            uint64_t ii;
            if (!init_flag) {
                *py = *px;
                ii = 1;
            } else {
                ii = 0;
            }
            nlcpy__global_u32 = (uint32_t)(*py);
@#pragma omp barrier
            const int it = omp_get_thread_num();
            const int nt = omp_get_max_threads();
            const uint64_t ist = (x->shape[0]-ii)*it/nt;
            const uint64_t ien = (x->shape[0]-ii)*(it+1)/nt;
            uint32_t tmp = (uint32_t)(*py);
#if "u32" eq "f32" || "u32" eq "f64"
            double is_there_nan = (isnan_u32(tmp)) ? 1.0 : 0.0;
#elif "u32" eq "i32" || "u32" eq "i64" || "u32" eq "bool"
            double is_there_nan = 0.0;
#else
#error Not Impletended
#endif
@#pragma _NEC ivdep
            for (i=ii+ist; i < ii+ien; i++) {
#if defined(maximum_reduce)
                tmp = (tmp > px[i*ix]) ? tmp : (uint32_t)px[i*ix];
#elif defined(minimum_reduce)
                tmp = (tmp < px[i*ix]) ? tmp : (uint32_t)px[i*ix];
#endif
#if defined(DTAG1_f32) || defined(DTAG1_f64)
                // NaN check:
                // is_there_nan += (float)isnan_@DTAG1@(px[i*ix]);
                @TYPE1@ xx;
                *(&xx) = px[i*ix];
                // The following line checks for NaN and it is in an unnatural way.
                // Here, NaN == NaN becomes False.
                // If there is one or more qNaN, signaling NaN (sNaN) occurs.
                // qNaN might not be detected qNaN by compiler optimizations.
                // However, we prioritize the performance.
                is_there_nan += (! (xx == px[i*ix]) ) ? 1.0 : 0.0;
#endif
            }
#if defined(DTAG1_f32) || defined(DTAG1_f64)
            if (is_there_nan != (float)0) {
                tmp = NAN;
                // In the following function call, PSW flags are manipulated to skip the intentional sNaN above.
                retrieve_fpe_flags(psw);
                *psw &= 0x0000003d;
                set_fpe_flags(*psw);
            }
#endif
@#pragma omp critical
            {
                @UNARY_OPERATOR@(tmp,nlcpy__global_u32,u32)
            }
@#pragma omp barrier
            *py = nlcpy__global_u32;
#else
        // Unvectorizable case
@#ifdef _OPENMP
@#pragma omp single
@#endif /* _OPENMP */
            {
                if (!init_flag) {
                    *py = *px;
                    i = 1;
                } else {
                    i = 0;
                }
@#pragma _NEC novector
                for (; i < x->shape[0]; i++) {
                    @UNARY_OPERATOR@(px[i*ix],*py,u32)
                }
            } /* omp single */
#endif
        } else {
@#ifdef _OPENMP
@#pragma omp single
@#endif /* _OPENMP */
            {
                Bint *pw = (Bint *)w->ve_adr;
                const uint64_t iw = w->strides[0]/w->itemsize;
                if (!init_flag) {
                    for (i = 0; i < x->shape[0]; i++) {
                        if (pw[i*iw]) {
                            *py = *px;
                            i++;
                            break;
                        }
                    }
                } else {
                    i = 0;
                }
#ifdef left_shift_reduce
@#pragma _NEC novector
#else
@#pragma _NEC ivdep
#endif
                for (; i < x->shape[0]; i++) {
                    if (pw[i*iw]) {
                        @UNARY_OPERATOR@(px[i*ix],*py,u32)
                    }
                }
            } /* omp single */
        }

/////////
// N-d //
/////////
    } else if (x->ndim <= NLCPY_MAXNDIM) {
        int64_t n_inner = x->ndim - 1;
        int64_t n_outer = 0;

        int64_t *idx = (int64_t*)alloca(sizeof(int64_t)*x->ndim);
        for (uint64_t i = 0; i < x->ndim; i++) {
            idx[i] = i;
        }
        if (x->ndim > 2) {
            int64_t max_idx[3], tmp;
            nlcpy__argnsort(x, max_idx, 3);
#ifndef add_reduce
            if (idx[max_idx[0]] == axis) {
                max_idx[0] = max_idx[1];
            }
#endif
            if (max_idx[0] != n_inner) {
                tmp = idx[n_inner];
                idx[n_inner] = idx[max_idx[0]];
                idx[max_idx[0]] = tmp;
            }
            if (idx[max_idx[1]] == axis) {
                max_idx[1] = max_idx[2];
            }
            if (idx[max_idx[1]] != n_outer) {
                tmp = idx[n_outer];
                idx[n_outer] = idx[max_idx[1]];
                idx[max_idx[1]] = tmp;
            }
        } else if (n_outer == axis) {
            idx[0] = 1;
            idx[1] = 0;
        }
        int64_t n_inner2 = idx[n_inner];
        int64_t n_outer2 = idx[n_outer];

@#ifdef _OPENMP
        const int nt = omp_get_num_threads();
        const int it = omp_get_thread_num();
@#else
        const int nt = 1;
        const int it = 0;
@#endif
        int64_t i;
        uint64_t adr_x = (uint64_t)x->ve_adr;
        uint64_t adr_y = (uint64_t)y->ve_adr;

        uint64_t ix = 0;
        uint64_t iy = 0;
        uint64_t iw = 0;
        uint64_t ix0 = x->strides[n_inner2] / x->itemsize;
        uint64_t iy0 = y->strides[n_inner2] / y->itemsize;
        uint64_t iw0 = 0;
        int64_t k = 0;
        int64_t *cnt_x = (int64_t*)alloca(sizeof(int64_t)*x->ndim);
        nlcpy__reset_coords(cnt_x, x->ndim);
        const int64_t lenm = x->shape[n_outer2];
        const int64_t cntm_s = lenm * it / nt;
        const int64_t cntm_e = lenm * (it + 1) / nt;
        if (!where_flag) {
            for (int64_t cntm = cntm_s; cntm < cntm_e; cntm++) {
                cnt_x[n_outer2] = cntm;
                ix = cntm * x->strides[n_outer2] / x->itemsize;
                iy = (n_outer2 == axis) ? 0 : cntm * y->strides[n_outer2] / y->itemsize;

                do {
                    if (n_inner2 == axis) {
                        // Note that a rational operation for complex and unsigned numbers occurs an error or warnning.
                        uint64_t ii;
                        if (!init_flag) {
                           py[iy] = px[ix];
                           ii = 1;
                        } else {
                           ii = 0;
                        }
#if ( defined(maximum_reduce) || defined(minimum_reduce) ) && ( defined(DTAG1_i32) || defined(DTAG1_i64) || defined(DTAG1_f32) || defined(DTAG1_f64) || defined(DTAG1_bool)) && ( "u32" eq "i32" || "u32" eq "i64" || "u32" eq "f32" || "u32" eq "f64" || "u32" eq "bool")
                        uint32_t tmp = py[iy];
#if "u32" eq "f32" || "u32" eq "f64"
                        float is_there_nan = (isnan_u32(py[iy])) ? 1.0 : 0.0;
#elif "u32" eq "i32" || "u32" eq "i64" || "u32" eq "bool"
                        float is_there_nan = 0.0;
#else
#error Not Impletended
#endif
@#pragma _NEC ivdep
                        for (i = ii; i < x->shape[n_inner2]; i++) {
#if defined(maximum_reduce)
                            tmp = (tmp > px[i*ix0+ix]) ? tmp : (uint32_t)px[i*ix0+ix];
#elif defined(minimum_reduce)
                            tmp = (tmp < px[i*ix0+ix]) ? tmp : (uint32_t)px[i*ix0+ix];
#endif
#if defined(DTAG1_f32) || defined(DTAG1_f64)
                            // NaN check:
                            //   is_there_nan += (float)isnan_@DTAG1@(px[i*ix0+ix]);
                            @TYPE1@ xx;
                            *(&xx) = px[i*ix0+ix];
                            // The following line checks for NaN and it is in an unnatural way.
                            // Here, NaN == NaN becomes False.
                            // If there is one or more qNaN, signaling NaN (sNaN) occurs.
                            // qNaN might not be detected qNaN by compiler optimizations.
                            // However, we prioritize the performance.
                            is_there_nan += (! (xx == px[i*ix0+ix]) ) ? 1.0 : 0.0;
#endif
                        }
#if defined(DTAG1_f32) || defined(DTAG1_f64)
                        if (is_there_nan != (float)0) {
                            tmp = NAN;
                            // In the following function call, PSW flags are manipulated to skip the intentional sNaN above.
                            retrieve_fpe_flags(psw);
                            *psw &= 0x0000003d;
                            set_fpe_flags(*psw);
                        }
#endif
                        py[iy] = tmp;
#else
                        // General case
#ifdef left_shift_reduce
@#pragma _NEC novector
#else
@#pragma _NEC ivdep
#endif
                        for (i = ii; i < x->shape[n_inner2]; i++) {
                            @UNARY_OPERATOR@(px[i*ix0+ix],py[iy],u32)
                        }
#endif
                    } else {
                        if (init_flag || cnt_x[axis] > 0) {
#ifdef left_shift_reduce
@#pragma _NEC novector
#else
@#pragma _NEC ivdep
#endif
                            for (i = 0; i < x->shape[n_inner2]; i++) {
                                @UNARY_OPERATOR@(px[i*ix0+ix],py[i*iy0+iy],u32)
                             }
                        } else {
                            for (i = 0; i < x->shape[n_inner2]; i++) {
                                py[i*iy0+iy] = @CAST_OPERATOR@(px[i*ix0+ix],@DTAG1@,u32)
                            }
                        }
                    }
                    for (k = n_inner - 1; k >= 1; k--) {
                        int64_t kk = idx[k];
                        if (kk == axis) {
                            if (++cnt_x[kk] < x->shape[kk]) {
                                ix += x->strides[kk] / x->itemsize;
                                break;
                            }
                            ix -= (x->strides[kk] / x->itemsize) * (x->shape[kk] - 1);
                        } else {
                            if (++cnt_x[kk] < x->shape[kk]) {
                                ix += x->strides[kk] / x->itemsize;
                                iy += y->strides[kk] / y->itemsize;
                                break;
                            }
                            ix -= (x->strides[kk] / x->itemsize) * (x->shape[kk] - 1);
                            iy -= (y->strides[kk] / y->itemsize) * (y->shape[kk] - 1);
                        }
                        cnt_x[kk] = 0;
                    }
                } while (k >= 1);
            }
        } else {
            Bint *pw = (Bint *)w->ve_adr;
            iw0 = w->strides[n_inner2] / w->itemsize;

            for (int64_t cntm = cntm_s; cntm < cntm_e; cntm++) {
                ix = cntm * x->strides[n_outer2] / x->itemsize;
                iy = (n_outer2 == axis) ? 0 : cntm * y->strides[n_outer2] / y->itemsize;
                iw = cntm * w->strides[n_outer2] / w->itemsize;

                do {
                    if (n_inner2 == axis) {
                        if (!init_flag) {
                            for (i = 0; i < w->shape[n_inner2]; i++) {
                                if (pw[i*iw0+iw]) {
                                    py[iy] = @CAST_OPERATOR@(px[i*ix0+ix],@DTAG1@,u32)
                                    i++;
                                    break;
                                }
                            }
                        } else {
                            i = 0;
                        }
#ifdef left_shift_reduce
@#pragma _NEC novector
#else
@#pragma _NEC ivdep
#endif
                        for (; i < x->shape[n_inner2]; i++) {
                            if (pw[i*iw0+iw]) {
                                @UNARY_OPERATOR@(px[i*ix0+ix],py[iy],u32)
                            }
                        }
                    } else {
                        if (init_flag || cnt_x[axis] > 0) {
#ifdef left_shift_reduce
@#pragma _NEC novector
#else
@#pragma _NEC ivdep
#endif
                            for (i = 0; i < x->shape[n_inner2]; i++) {
                                if (pw[i*iw0+iw]) {
                                    @UNARY_OPERATOR@(px[i*ix0+ix],py[i*iy0+iy],u32)
                                }
                            }
                        } else {
                            for (i = 0; i < x->shape[n_inner2]; i++) {
                                if (pw[i*iw0+iw]) {
                                    py[i*iy0+iy] = @CAST_OPERATOR@(px[i*ix0+ix],@DTAG1@,u32)
                                }
                            }
                        }
                    }
                    for (k = n_inner - 1; k >= 1; k--) {
                        int64_t kk = idx[k];
                        if (kk == axis) {
                            if (++cnt_x[kk] < x->shape[kk]) {
                                ix += x->strides[kk] / x->itemsize;
                                iw += w->strides[kk] / w->itemsize;
                                break;
                            }
                            ix -= (x->strides[kk] / x->itemsize) * (x->shape[kk] - 1);
                            iw -= (w->strides[kk] / w->itemsize) * (x->shape[kk] - 1);
                        } else {
                            if (++cnt_x[kk] < x->shape[kk]) {
                                ix += x->strides[kk] / x->itemsize;
                                iy += y->strides[kk] / y->itemsize;
                                iw += w->strides[kk] / w->itemsize;
                                break;
                            }
                            ix -= (x->strides[kk] / x->itemsize) * (x->shape[kk] - 1);
                            iy -= (y->strides[kk] / y->itemsize) * (x->shape[kk] - 1);
                            iw -= (w->strides[kk] / w->itemsize) * (x->shape[kk] - 1);
                        }
                        cnt_x[kk] = 0;
                    }
                } while (k >= 1);
            }
        }
    } else {
        return (uint64_t)NLCPY_ERROR_NDIM;
    }
    retrieve_fpe_flags(psw);
    return (uint64_t)NLCPY_ERROR_OK;
}


#endif
#if defined(DTAG_u64)

uint64_t FILENAME_@DTAG1@_u64(ve_array *x, ve_array *y, int32_t axis, int32_t init_flag,
                     ve_array *initial, int32_t where_flag, ve_array *where, int32_t *psw)
{
    @TYPE1@ *px = (@TYPE1@ *)nlcpy__get_ptr(x);
    if (px == NULL) return NLCPY_ERROR_MEMORY;
    uint64_t *py = (uint64_t *)nlcpy__get_ptr(y);
    if (py == NULL) return NLCPY_ERROR_MEMORY;
    uint64_t *pi = (uint64_t *)nlcpy__get_ptr(initial);
    if (pi == NULL) return NLCPY_ERROR_MEMORY;
    ve_array *w = where;
    uint64_t init_val = (uint64_t)(*pi);

    // initialize
    if (init_flag) {
@#ifdef _OPENMP
@#pragma omp for
@#endif /* _OPENMP */
        for (uint64_t i=0; i<y->size; i++) py[i] = init_val;
    }
    else {
@#ifdef _OPENMP
@#pragma omp for
@#endif /* _OPENMP */
        for (uint64_t i=0; i<y->size; i++) py[i] = 0;
    }

/////////
// 0-d //
/////////
    if (x->ndim == 0){
@#ifdef _OPENMP
@#pragma omp single
@#endif /* _OPENMP */
{
        if (!where_flag) {
            if (!init_flag) {
                *py = *px;
            }
            else{
                @UNARY_OPERATOR@(*px,*py,u64)
            }
        } else {
            Bint *pw = (Bint *)w->ve_adr;
            if (*pw) {
                if (!init_flag) {
                    *py = *px;
                }
                else{
                    @UNARY_OPERATOR@(*px,*py,u64)
                }
            }
        }
} /* omp single */

/////////
// 1-d //
/////////
    } else if (x->ndim == 1){
        uint64_t i;
        uint64_t iw0 = 0;
        const uint64_t ix = x->strides[0]/x->itemsize;
        if (!where_flag) {
#if defined(add_reduce) || defined(multiply_reduce)
            uint64_t tmp;
#if defined(add_reduce)
            nlcpy__global_u64 = 0;
@#pragma omp barrier
            tmp = 0;
#elif defined(multiply_reduce)
            nlcpy__global_u64 = 1;
@#pragma omp barrier
            tmp = 1;
#else
#error add_reduce or minimum_reduce must be defined.
#endif
            uint64_t ii;
            if (!init_flag) {
                *py = *px;
                ii = 1;
            } else {
                ii = 0;
            }
            const int it = omp_get_thread_num();
            const int nt = omp_get_max_threads();
            const uint64_t ist = (x->shape[0]-ii)*it/nt;
            const uint64_t ien = (x->shape[0]-ii)*(it+1)/nt;
            if (it==0) tmp = (uint64_t)(*py);
@#pragma _NEC ivdep
            for (i=ii+ist; i < ii+ien; i++) {
                @UNARY_OPERATOR@(px[i*ix],tmp,u64)
            }
@#pragma _NEC novector
            for (i=0; i < nt; i++) {
                if (i==it) {
                    @UNARY_OPERATOR@(tmp,nlcpy__global_u64,u64)
                }
@#pragma omp barrier
            }
            *py = nlcpy__global_u64;
#elif ( defined(maximum_reduce) || defined(minimum_reduce) ) && ( defined(DTAG1_i32) || defined(DTAG1_i64) || defined(DTAG1_f32) || defined(DTAG1_f64) || defined(DTAG1_bool)) && ( "u64" eq "i32" || "u64" eq "i64" || "u64" eq "f32" || "u64" eq "f64" || "u64" eq "bool")
            // Note that rational operations for complex and unsigned numbers occurs an error or warnning.
            uint64_t ii;
            if (!init_flag) {
                *py = *px;
                ii = 1;
            } else {
                ii = 0;
            }
            nlcpy__global_u64 = (uint64_t)(*py);
@#pragma omp barrier
            const int it = omp_get_thread_num();
            const int nt = omp_get_max_threads();
            const uint64_t ist = (x->shape[0]-ii)*it/nt;
            const uint64_t ien = (x->shape[0]-ii)*(it+1)/nt;
            uint64_t tmp = (uint64_t)(*py);
#if "u64" eq "f32" || "u64" eq "f64"
            double is_there_nan = (isnan_u64(tmp)) ? 1.0 : 0.0;
#elif "u64" eq "i32" || "u64" eq "i64" || "u64" eq "bool"
            double is_there_nan = 0.0;
#else
#error Not Impletended
#endif
@#pragma _NEC ivdep
            for (i=ii+ist; i < ii+ien; i++) {
#if defined(maximum_reduce)
                tmp = (tmp > px[i*ix]) ? tmp : (uint64_t)px[i*ix];
#elif defined(minimum_reduce)
                tmp = (tmp < px[i*ix]) ? tmp : (uint64_t)px[i*ix];
#endif
#if defined(DTAG1_f32) || defined(DTAG1_f64)
                // NaN check:
                // is_there_nan += (float)isnan_@DTAG1@(px[i*ix]);
                @TYPE1@ xx;
                *(&xx) = px[i*ix];
                // The following line checks for NaN and it is in an unnatural way.
                // Here, NaN == NaN becomes False.
                // If there is one or more qNaN, signaling NaN (sNaN) occurs.
                // qNaN might not be detected qNaN by compiler optimizations.
                // However, we prioritize the performance.
                is_there_nan += (! (xx == px[i*ix]) ) ? 1.0 : 0.0;
#endif
            }
#if defined(DTAG1_f32) || defined(DTAG1_f64)
            if (is_there_nan != (float)0) {
                tmp = NAN;
                // In the following function call, PSW flags are manipulated to skip the intentional sNaN above.
                retrieve_fpe_flags(psw);
                *psw &= 0x0000003d;
                set_fpe_flags(*psw);
            }
#endif
@#pragma omp critical
            {
                @UNARY_OPERATOR@(tmp,nlcpy__global_u64,u64)
            }
@#pragma omp barrier
            *py = nlcpy__global_u64;
#else
        // Unvectorizable case
@#ifdef _OPENMP
@#pragma omp single
@#endif /* _OPENMP */
            {
                if (!init_flag) {
                    *py = *px;
                    i = 1;
                } else {
                    i = 0;
                }
@#pragma _NEC novector
                for (; i < x->shape[0]; i++) {
                    @UNARY_OPERATOR@(px[i*ix],*py,u64)
                }
            } /* omp single */
#endif
        } else {
@#ifdef _OPENMP
@#pragma omp single
@#endif /* _OPENMP */
            {
                Bint *pw = (Bint *)w->ve_adr;
                const uint64_t iw = w->strides[0]/w->itemsize;
                if (!init_flag) {
                    for (i = 0; i < x->shape[0]; i++) {
                        if (pw[i*iw]) {
                            *py = *px;
                            i++;
                            break;
                        }
                    }
                } else {
                    i = 0;
                }
#ifdef left_shift_reduce
@#pragma _NEC novector
#else
@#pragma _NEC ivdep
#endif
                for (; i < x->shape[0]; i++) {
                    if (pw[i*iw]) {
                        @UNARY_OPERATOR@(px[i*ix],*py,u64)
                    }
                }
            } /* omp single */
        }

/////////
// N-d //
/////////
    } else if (x->ndim <= NLCPY_MAXNDIM) {
        int64_t n_inner = x->ndim - 1;
        int64_t n_outer = 0;

        int64_t *idx = (int64_t*)alloca(sizeof(int64_t)*x->ndim);
        for (uint64_t i = 0; i < x->ndim; i++) {
            idx[i] = i;
        }
        if (x->ndim > 2) {
            int64_t max_idx[3], tmp;
            nlcpy__argnsort(x, max_idx, 3);
#ifndef add_reduce
            if (idx[max_idx[0]] == axis) {
                max_idx[0] = max_idx[1];
            }
#endif
            if (max_idx[0] != n_inner) {
                tmp = idx[n_inner];
                idx[n_inner] = idx[max_idx[0]];
                idx[max_idx[0]] = tmp;
            }
            if (idx[max_idx[1]] == axis) {
                max_idx[1] = max_idx[2];
            }
            if (idx[max_idx[1]] != n_outer) {
                tmp = idx[n_outer];
                idx[n_outer] = idx[max_idx[1]];
                idx[max_idx[1]] = tmp;
            }
        } else if (n_outer == axis) {
            idx[0] = 1;
            idx[1] = 0;
        }
        int64_t n_inner2 = idx[n_inner];
        int64_t n_outer2 = idx[n_outer];

@#ifdef _OPENMP
        const int nt = omp_get_num_threads();
        const int it = omp_get_thread_num();
@#else
        const int nt = 1;
        const int it = 0;
@#endif
        int64_t i;
        uint64_t adr_x = (uint64_t)x->ve_adr;
        uint64_t adr_y = (uint64_t)y->ve_adr;

        uint64_t ix = 0;
        uint64_t iy = 0;
        uint64_t iw = 0;
        uint64_t ix0 = x->strides[n_inner2] / x->itemsize;
        uint64_t iy0 = y->strides[n_inner2] / y->itemsize;
        uint64_t iw0 = 0;
        int64_t k = 0;
        int64_t *cnt_x = (int64_t*)alloca(sizeof(int64_t)*x->ndim);
        nlcpy__reset_coords(cnt_x, x->ndim);
        const int64_t lenm = x->shape[n_outer2];
        const int64_t cntm_s = lenm * it / nt;
        const int64_t cntm_e = lenm * (it + 1) / nt;
        if (!where_flag) {
            for (int64_t cntm = cntm_s; cntm < cntm_e; cntm++) {
                cnt_x[n_outer2] = cntm;
                ix = cntm * x->strides[n_outer2] / x->itemsize;
                iy = (n_outer2 == axis) ? 0 : cntm * y->strides[n_outer2] / y->itemsize;

                do {
                    if (n_inner2 == axis) {
                        // Note that a rational operation for complex and unsigned numbers occurs an error or warnning.
                        uint64_t ii;
                        if (!init_flag) {
                           py[iy] = px[ix];
                           ii = 1;
                        } else {
                           ii = 0;
                        }
#if ( defined(maximum_reduce) || defined(minimum_reduce) ) && ( defined(DTAG1_i32) || defined(DTAG1_i64) || defined(DTAG1_f32) || defined(DTAG1_f64) || defined(DTAG1_bool)) && ( "u64" eq "i32" || "u64" eq "i64" || "u64" eq "f32" || "u64" eq "f64" || "u64" eq "bool")
                        uint64_t tmp = py[iy];
#if "u64" eq "f32" || "u64" eq "f64"
                        float is_there_nan = (isnan_u64(py[iy])) ? 1.0 : 0.0;
#elif "u64" eq "i32" || "u64" eq "i64" || "u64" eq "bool"
                        float is_there_nan = 0.0;
#else
#error Not Impletended
#endif
@#pragma _NEC ivdep
                        for (i = ii; i < x->shape[n_inner2]; i++) {
#if defined(maximum_reduce)
                            tmp = (tmp > px[i*ix0+ix]) ? tmp : (uint64_t)px[i*ix0+ix];
#elif defined(minimum_reduce)
                            tmp = (tmp < px[i*ix0+ix]) ? tmp : (uint64_t)px[i*ix0+ix];
#endif
#if defined(DTAG1_f32) || defined(DTAG1_f64)
                            // NaN check:
                            //   is_there_nan += (float)isnan_@DTAG1@(px[i*ix0+ix]);
                            @TYPE1@ xx;
                            *(&xx) = px[i*ix0+ix];
                            // The following line checks for NaN and it is in an unnatural way.
                            // Here, NaN == NaN becomes False.
                            // If there is one or more qNaN, signaling NaN (sNaN) occurs.
                            // qNaN might not be detected qNaN by compiler optimizations.
                            // However, we prioritize the performance.
                            is_there_nan += (! (xx == px[i*ix0+ix]) ) ? 1.0 : 0.0;
#endif
                        }
#if defined(DTAG1_f32) || defined(DTAG1_f64)
                        if (is_there_nan != (float)0) {
                            tmp = NAN;
                            // In the following function call, PSW flags are manipulated to skip the intentional sNaN above.
                            retrieve_fpe_flags(psw);
                            *psw &= 0x0000003d;
                            set_fpe_flags(*psw);
                        }
#endif
                        py[iy] = tmp;
#else
                        // General case
#ifdef left_shift_reduce
@#pragma _NEC novector
#else
@#pragma _NEC ivdep
#endif
                        for (i = ii; i < x->shape[n_inner2]; i++) {
                            @UNARY_OPERATOR@(px[i*ix0+ix],py[iy],u64)
                        }
#endif
                    } else {
                        if (init_flag || cnt_x[axis] > 0) {
#ifdef left_shift_reduce
@#pragma _NEC novector
#else
@#pragma _NEC ivdep
#endif
                            for (i = 0; i < x->shape[n_inner2]; i++) {
                                @UNARY_OPERATOR@(px[i*ix0+ix],py[i*iy0+iy],u64)
                             }
                        } else {
                            for (i = 0; i < x->shape[n_inner2]; i++) {
                                py[i*iy0+iy] = @CAST_OPERATOR@(px[i*ix0+ix],@DTAG1@,u64)
                            }
                        }
                    }
                    for (k = n_inner - 1; k >= 1; k--) {
                        int64_t kk = idx[k];
                        if (kk == axis) {
                            if (++cnt_x[kk] < x->shape[kk]) {
                                ix += x->strides[kk] / x->itemsize;
                                break;
                            }
                            ix -= (x->strides[kk] / x->itemsize) * (x->shape[kk] - 1);
                        } else {
                            if (++cnt_x[kk] < x->shape[kk]) {
                                ix += x->strides[kk] / x->itemsize;
                                iy += y->strides[kk] / y->itemsize;
                                break;
                            }
                            ix -= (x->strides[kk] / x->itemsize) * (x->shape[kk] - 1);
                            iy -= (y->strides[kk] / y->itemsize) * (y->shape[kk] - 1);
                        }
                        cnt_x[kk] = 0;
                    }
                } while (k >= 1);
            }
        } else {
            Bint *pw = (Bint *)w->ve_adr;
            iw0 = w->strides[n_inner2] / w->itemsize;

            for (int64_t cntm = cntm_s; cntm < cntm_e; cntm++) {
                ix = cntm * x->strides[n_outer2] / x->itemsize;
                iy = (n_outer2 == axis) ? 0 : cntm * y->strides[n_outer2] / y->itemsize;
                iw = cntm * w->strides[n_outer2] / w->itemsize;

                do {
                    if (n_inner2 == axis) {
                        if (!init_flag) {
                            for (i = 0; i < w->shape[n_inner2]; i++) {
                                if (pw[i*iw0+iw]) {
                                    py[iy] = @CAST_OPERATOR@(px[i*ix0+ix],@DTAG1@,u64)
                                    i++;
                                    break;
                                }
                            }
                        } else {
                            i = 0;
                        }
#ifdef left_shift_reduce
@#pragma _NEC novector
#else
@#pragma _NEC ivdep
#endif
                        for (; i < x->shape[n_inner2]; i++) {
                            if (pw[i*iw0+iw]) {
                                @UNARY_OPERATOR@(px[i*ix0+ix],py[iy],u64)
                            }
                        }
                    } else {
                        if (init_flag || cnt_x[axis] > 0) {
#ifdef left_shift_reduce
@#pragma _NEC novector
#else
@#pragma _NEC ivdep
#endif
                            for (i = 0; i < x->shape[n_inner2]; i++) {
                                if (pw[i*iw0+iw]) {
                                    @UNARY_OPERATOR@(px[i*ix0+ix],py[i*iy0+iy],u64)
                                }
                            }
                        } else {
                            for (i = 0; i < x->shape[n_inner2]; i++) {
                                if (pw[i*iw0+iw]) {
                                    py[i*iy0+iy] = @CAST_OPERATOR@(px[i*ix0+ix],@DTAG1@,u64)
                                }
                            }
                        }
                    }
                    for (k = n_inner - 1; k >= 1; k--) {
                        int64_t kk = idx[k];
                        if (kk == axis) {
                            if (++cnt_x[kk] < x->shape[kk]) {
                                ix += x->strides[kk] / x->itemsize;
                                iw += w->strides[kk] / w->itemsize;
                                break;
                            }
                            ix -= (x->strides[kk] / x->itemsize) * (x->shape[kk] - 1);
                            iw -= (w->strides[kk] / w->itemsize) * (x->shape[kk] - 1);
                        } else {
                            if (++cnt_x[kk] < x->shape[kk]) {
                                ix += x->strides[kk] / x->itemsize;
                                iy += y->strides[kk] / y->itemsize;
                                iw += w->strides[kk] / w->itemsize;
                                break;
                            }
                            ix -= (x->strides[kk] / x->itemsize) * (x->shape[kk] - 1);
                            iy -= (y->strides[kk] / y->itemsize) * (x->shape[kk] - 1);
                            iw -= (w->strides[kk] / w->itemsize) * (x->shape[kk] - 1);
                        }
                        cnt_x[kk] = 0;
                    }
                } while (k >= 1);
            }
        }
    } else {
        return (uint64_t)NLCPY_ERROR_NDIM;
    }
    retrieve_fpe_flags(psw);
    return (uint64_t)NLCPY_ERROR_OK;
}


#endif
#if defined(DTAG_f32)

uint64_t FILENAME_@DTAG1@_f32(ve_array *x, ve_array *y, int32_t axis, int32_t init_flag,
                     ve_array *initial, int32_t where_flag, ve_array *where, int32_t *psw)
{
    @TYPE1@ *px = (@TYPE1@ *)nlcpy__get_ptr(x);
    if (px == NULL) return NLCPY_ERROR_MEMORY;
    float *py = (float *)nlcpy__get_ptr(y);
    if (py == NULL) return NLCPY_ERROR_MEMORY;
    float *pi = (float *)nlcpy__get_ptr(initial);
    if (pi == NULL) return NLCPY_ERROR_MEMORY;
    ve_array *w = where;
    float init_val = (float)(*pi);

    // initialize
    if (init_flag) {
@#ifdef _OPENMP
@#pragma omp for
@#endif /* _OPENMP */
        for (uint64_t i=0; i<y->size; i++) py[i] = init_val;
    }
    else {
@#ifdef _OPENMP
@#pragma omp for
@#endif /* _OPENMP */
        for (uint64_t i=0; i<y->size; i++) py[i] = 0;
    }

/////////
// 0-d //
/////////
    if (x->ndim == 0){
@#ifdef _OPENMP
@#pragma omp single
@#endif /* _OPENMP */
{
        if (!where_flag) {
            if (!init_flag) {
                *py = *px;
            }
            else{
                @UNARY_OPERATOR@(*px,*py,f32)
            }
        } else {
            Bint *pw = (Bint *)w->ve_adr;
            if (*pw) {
                if (!init_flag) {
                    *py = *px;
                }
                else{
                    @UNARY_OPERATOR@(*px,*py,f32)
                }
            }
        }
} /* omp single */

/////////
// 1-d //
/////////
    } else if (x->ndim == 1){
        uint64_t i;
        uint64_t iw0 = 0;
        const uint64_t ix = x->strides[0]/x->itemsize;
        if (!where_flag) {
#if defined(add_reduce) || defined(multiply_reduce)
            float tmp;
#if defined(add_reduce)
            nlcpy__global_f32 = 0;
@#pragma omp barrier
            tmp = 0;
#elif defined(multiply_reduce)
            nlcpy__global_f32 = 1;
@#pragma omp barrier
            tmp = 1;
#else
#error add_reduce or minimum_reduce must be defined.
#endif
            uint64_t ii;
            if (!init_flag) {
                *py = *px;
                ii = 1;
            } else {
                ii = 0;
            }
            const int it = omp_get_thread_num();
            const int nt = omp_get_max_threads();
            const uint64_t ist = (x->shape[0]-ii)*it/nt;
            const uint64_t ien = (x->shape[0]-ii)*(it+1)/nt;
            if (it==0) tmp = (float)(*py);
@#pragma _NEC ivdep
            for (i=ii+ist; i < ii+ien; i++) {
                @UNARY_OPERATOR@(px[i*ix],tmp,f32)
            }
@#pragma _NEC novector
            for (i=0; i < nt; i++) {
                if (i==it) {
                    @UNARY_OPERATOR@(tmp,nlcpy__global_f32,f32)
                }
@#pragma omp barrier
            }
            *py = nlcpy__global_f32;
#elif ( defined(maximum_reduce) || defined(minimum_reduce) ) && ( defined(DTAG1_i32) || defined(DTAG1_i64) || defined(DTAG1_f32) || defined(DTAG1_f64) || defined(DTAG1_bool)) && ( "f32" eq "i32" || "f32" eq "i64" || "f32" eq "f32" || "f32" eq "f64" || "f32" eq "bool")
            // Note that rational operations for complex and unsigned numbers occurs an error or warnning.
            uint64_t ii;
            if (!init_flag) {
                *py = *px;
                ii = 1;
            } else {
                ii = 0;
            }
            nlcpy__global_f32 = (float)(*py);
@#pragma omp barrier
            const int it = omp_get_thread_num();
            const int nt = omp_get_max_threads();
            const uint64_t ist = (x->shape[0]-ii)*it/nt;
            const uint64_t ien = (x->shape[0]-ii)*(it+1)/nt;
            float tmp = (float)(*py);
#if "f32" eq "f32" || "f32" eq "f64"
            double is_there_nan = (isnan_f32(tmp)) ? 1.0 : 0.0;
#elif "f32" eq "i32" || "f32" eq "i64" || "f32" eq "bool"
            double is_there_nan = 0.0;
#else
#error Not Impletended
#endif
@#pragma _NEC ivdep
            for (i=ii+ist; i < ii+ien; i++) {
#if defined(maximum_reduce)
                tmp = (tmp > px[i*ix]) ? tmp : (float)px[i*ix];
#elif defined(minimum_reduce)
                tmp = (tmp < px[i*ix]) ? tmp : (float)px[i*ix];
#endif
#if defined(DTAG1_f32) || defined(DTAG1_f64)
                // NaN check:
                // is_there_nan += (float)isnan_@DTAG1@(px[i*ix]);
                @TYPE1@ xx;
                *(&xx) = px[i*ix];
                // The following line checks for NaN and it is in an unnatural way.
                // Here, NaN == NaN becomes False.
                // If there is one or more qNaN, signaling NaN (sNaN) occurs.
                // qNaN might not be detected qNaN by compiler optimizations.
                // However, we prioritize the performance.
                is_there_nan += (! (xx == px[i*ix]) ) ? 1.0 : 0.0;
#endif
            }
#if defined(DTAG1_f32) || defined(DTAG1_f64)
            if (is_there_nan != (float)0) {
                tmp = NAN;
                // In the following function call, PSW flags are manipulated to skip the intentional sNaN above.
                retrieve_fpe_flags(psw);
                *psw &= 0x0000003d;
                set_fpe_flags(*psw);
            }
#endif
@#pragma omp critical
            {
                @UNARY_OPERATOR@(tmp,nlcpy__global_f32,f32)
            }
@#pragma omp barrier
            *py = nlcpy__global_f32;
#else
        // Unvectorizable case
@#ifdef _OPENMP
@#pragma omp single
@#endif /* _OPENMP */
            {
                if (!init_flag) {
                    *py = *px;
                    i = 1;
                } else {
                    i = 0;
                }
@#pragma _NEC novector
                for (; i < x->shape[0]; i++) {
                    @UNARY_OPERATOR@(px[i*ix],*py,f32)
                }
            } /* omp single */
#endif
        } else {
@#ifdef _OPENMP
@#pragma omp single
@#endif /* _OPENMP */
            {
                Bint *pw = (Bint *)w->ve_adr;
                const uint64_t iw = w->strides[0]/w->itemsize;
                if (!init_flag) {
                    for (i = 0; i < x->shape[0]; i++) {
                        if (pw[i*iw]) {
                            *py = *px;
                            i++;
                            break;
                        }
                    }
                } else {
                    i = 0;
                }
#ifdef left_shift_reduce
@#pragma _NEC novector
#else
@#pragma _NEC ivdep
#endif
                for (; i < x->shape[0]; i++) {
                    if (pw[i*iw]) {
                        @UNARY_OPERATOR@(px[i*ix],*py,f32)
                    }
                }
            } /* omp single */
        }

/////////
// N-d //
/////////
    } else if (x->ndim <= NLCPY_MAXNDIM) {
        int64_t n_inner = x->ndim - 1;
        int64_t n_outer = 0;

        int64_t *idx = (int64_t*)alloca(sizeof(int64_t)*x->ndim);
        for (uint64_t i = 0; i < x->ndim; i++) {
            idx[i] = i;
        }
        if (x->ndim > 2) {
            int64_t max_idx[3], tmp;
            nlcpy__argnsort(x, max_idx, 3);
#ifndef add_reduce
            if (idx[max_idx[0]] == axis) {
                max_idx[0] = max_idx[1];
            }
#endif
            if (max_idx[0] != n_inner) {
                tmp = idx[n_inner];
                idx[n_inner] = idx[max_idx[0]];
                idx[max_idx[0]] = tmp;
            }
            if (idx[max_idx[1]] == axis) {
                max_idx[1] = max_idx[2];
            }
            if (idx[max_idx[1]] != n_outer) {
                tmp = idx[n_outer];
                idx[n_outer] = idx[max_idx[1]];
                idx[max_idx[1]] = tmp;
            }
        } else if (n_outer == axis) {
            idx[0] = 1;
            idx[1] = 0;
        }
        int64_t n_inner2 = idx[n_inner];
        int64_t n_outer2 = idx[n_outer];

@#ifdef _OPENMP
        const int nt = omp_get_num_threads();
        const int it = omp_get_thread_num();
@#else
        const int nt = 1;
        const int it = 0;
@#endif
        int64_t i;
        uint64_t adr_x = (uint64_t)x->ve_adr;
        uint64_t adr_y = (uint64_t)y->ve_adr;

        uint64_t ix = 0;
        uint64_t iy = 0;
        uint64_t iw = 0;
        uint64_t ix0 = x->strides[n_inner2] / x->itemsize;
        uint64_t iy0 = y->strides[n_inner2] / y->itemsize;
        uint64_t iw0 = 0;
        int64_t k = 0;
        int64_t *cnt_x = (int64_t*)alloca(sizeof(int64_t)*x->ndim);
        nlcpy__reset_coords(cnt_x, x->ndim);
        const int64_t lenm = x->shape[n_outer2];
        const int64_t cntm_s = lenm * it / nt;
        const int64_t cntm_e = lenm * (it + 1) / nt;
        if (!where_flag) {
            for (int64_t cntm = cntm_s; cntm < cntm_e; cntm++) {
                cnt_x[n_outer2] = cntm;
                ix = cntm * x->strides[n_outer2] / x->itemsize;
                iy = (n_outer2 == axis) ? 0 : cntm * y->strides[n_outer2] / y->itemsize;

                do {
                    if (n_inner2 == axis) {
                        // Note that a rational operation for complex and unsigned numbers occurs an error or warnning.
                        uint64_t ii;
                        if (!init_flag) {
                           py[iy] = px[ix];
                           ii = 1;
                        } else {
                           ii = 0;
                        }
#if ( defined(maximum_reduce) || defined(minimum_reduce) ) && ( defined(DTAG1_i32) || defined(DTAG1_i64) || defined(DTAG1_f32) || defined(DTAG1_f64) || defined(DTAG1_bool)) && ( "f32" eq "i32" || "f32" eq "i64" || "f32" eq "f32" || "f32" eq "f64" || "f32" eq "bool")
                        float tmp = py[iy];
#if "f32" eq "f32" || "f32" eq "f64"
                        float is_there_nan = (isnan_f32(py[iy])) ? 1.0 : 0.0;
#elif "f32" eq "i32" || "f32" eq "i64" || "f32" eq "bool"
                        float is_there_nan = 0.0;
#else
#error Not Impletended
#endif
@#pragma _NEC ivdep
                        for (i = ii; i < x->shape[n_inner2]; i++) {
#if defined(maximum_reduce)
                            tmp = (tmp > px[i*ix0+ix]) ? tmp : (float)px[i*ix0+ix];
#elif defined(minimum_reduce)
                            tmp = (tmp < px[i*ix0+ix]) ? tmp : (float)px[i*ix0+ix];
#endif
#if defined(DTAG1_f32) || defined(DTAG1_f64)
                            // NaN check:
                            //   is_there_nan += (float)isnan_@DTAG1@(px[i*ix0+ix]);
                            @TYPE1@ xx;
                            *(&xx) = px[i*ix0+ix];
                            // The following line checks for NaN and it is in an unnatural way.
                            // Here, NaN == NaN becomes False.
                            // If there is one or more qNaN, signaling NaN (sNaN) occurs.
                            // qNaN might not be detected qNaN by compiler optimizations.
                            // However, we prioritize the performance.
                            is_there_nan += (! (xx == px[i*ix0+ix]) ) ? 1.0 : 0.0;
#endif
                        }
#if defined(DTAG1_f32) || defined(DTAG1_f64)
                        if (is_there_nan != (float)0) {
                            tmp = NAN;
                            // In the following function call, PSW flags are manipulated to skip the intentional sNaN above.
                            retrieve_fpe_flags(psw);
                            *psw &= 0x0000003d;
                            set_fpe_flags(*psw);
                        }
#endif
                        py[iy] = tmp;
#else
                        // General case
#ifdef left_shift_reduce
@#pragma _NEC novector
#else
@#pragma _NEC ivdep
#endif
                        for (i = ii; i < x->shape[n_inner2]; i++) {
                            @UNARY_OPERATOR@(px[i*ix0+ix],py[iy],f32)
                        }
#endif
                    } else {
                        if (init_flag || cnt_x[axis] > 0) {
#ifdef left_shift_reduce
@#pragma _NEC novector
#else
@#pragma _NEC ivdep
#endif
                            for (i = 0; i < x->shape[n_inner2]; i++) {
                                @UNARY_OPERATOR@(px[i*ix0+ix],py[i*iy0+iy],f32)
                             }
                        } else {
                            for (i = 0; i < x->shape[n_inner2]; i++) {
                                py[i*iy0+iy] = @CAST_OPERATOR@(px[i*ix0+ix],@DTAG1@,f32)
                            }
                        }
                    }
                    for (k = n_inner - 1; k >= 1; k--) {
                        int64_t kk = idx[k];
                        if (kk == axis) {
                            if (++cnt_x[kk] < x->shape[kk]) {
                                ix += x->strides[kk] / x->itemsize;
                                break;
                            }
                            ix -= (x->strides[kk] / x->itemsize) * (x->shape[kk] - 1);
                        } else {
                            if (++cnt_x[kk] < x->shape[kk]) {
                                ix += x->strides[kk] / x->itemsize;
                                iy += y->strides[kk] / y->itemsize;
                                break;
                            }
                            ix -= (x->strides[kk] / x->itemsize) * (x->shape[kk] - 1);
                            iy -= (y->strides[kk] / y->itemsize) * (y->shape[kk] - 1);
                        }
                        cnt_x[kk] = 0;
                    }
                } while (k >= 1);
            }
        } else {
            Bint *pw = (Bint *)w->ve_adr;
            iw0 = w->strides[n_inner2] / w->itemsize;

            for (int64_t cntm = cntm_s; cntm < cntm_e; cntm++) {
                ix = cntm * x->strides[n_outer2] / x->itemsize;
                iy = (n_outer2 == axis) ? 0 : cntm * y->strides[n_outer2] / y->itemsize;
                iw = cntm * w->strides[n_outer2] / w->itemsize;

                do {
                    if (n_inner2 == axis) {
                        if (!init_flag) {
                            for (i = 0; i < w->shape[n_inner2]; i++) {
                                if (pw[i*iw0+iw]) {
                                    py[iy] = @CAST_OPERATOR@(px[i*ix0+ix],@DTAG1@,f32)
                                    i++;
                                    break;
                                }
                            }
                        } else {
                            i = 0;
                        }
#ifdef left_shift_reduce
@#pragma _NEC novector
#else
@#pragma _NEC ivdep
#endif
                        for (; i < x->shape[n_inner2]; i++) {
                            if (pw[i*iw0+iw]) {
                                @UNARY_OPERATOR@(px[i*ix0+ix],py[iy],f32)
                            }
                        }
                    } else {
                        if (init_flag || cnt_x[axis] > 0) {
#ifdef left_shift_reduce
@#pragma _NEC novector
#else
@#pragma _NEC ivdep
#endif
                            for (i = 0; i < x->shape[n_inner2]; i++) {
                                if (pw[i*iw0+iw]) {
                                    @UNARY_OPERATOR@(px[i*ix0+ix],py[i*iy0+iy],f32)
                                }
                            }
                        } else {
                            for (i = 0; i < x->shape[n_inner2]; i++) {
                                if (pw[i*iw0+iw]) {
                                    py[i*iy0+iy] = @CAST_OPERATOR@(px[i*ix0+ix],@DTAG1@,f32)
                                }
                            }
                        }
                    }
                    for (k = n_inner - 1; k >= 1; k--) {
                        int64_t kk = idx[k];
                        if (kk == axis) {
                            if (++cnt_x[kk] < x->shape[kk]) {
                                ix += x->strides[kk] / x->itemsize;
                                iw += w->strides[kk] / w->itemsize;
                                break;
                            }
                            ix -= (x->strides[kk] / x->itemsize) * (x->shape[kk] - 1);
                            iw -= (w->strides[kk] / w->itemsize) * (x->shape[kk] - 1);
                        } else {
                            if (++cnt_x[kk] < x->shape[kk]) {
                                ix += x->strides[kk] / x->itemsize;
                                iy += y->strides[kk] / y->itemsize;
                                iw += w->strides[kk] / w->itemsize;
                                break;
                            }
                            ix -= (x->strides[kk] / x->itemsize) * (x->shape[kk] - 1);
                            iy -= (y->strides[kk] / y->itemsize) * (x->shape[kk] - 1);
                            iw -= (w->strides[kk] / w->itemsize) * (x->shape[kk] - 1);
                        }
                        cnt_x[kk] = 0;
                    }
                } while (k >= 1);
            }
        }
    } else {
        return (uint64_t)NLCPY_ERROR_NDIM;
    }
    retrieve_fpe_flags(psw);
    return (uint64_t)NLCPY_ERROR_OK;
}


#endif
#if defined(DTAG_f64)

uint64_t FILENAME_@DTAG1@_f64(ve_array *x, ve_array *y, int32_t axis, int32_t init_flag,
                     ve_array *initial, int32_t where_flag, ve_array *where, int32_t *psw)
{
    @TYPE1@ *px = (@TYPE1@ *)nlcpy__get_ptr(x);
    if (px == NULL) return NLCPY_ERROR_MEMORY;
    double *py = (double *)nlcpy__get_ptr(y);
    if (py == NULL) return NLCPY_ERROR_MEMORY;
    double *pi = (double *)nlcpy__get_ptr(initial);
    if (pi == NULL) return NLCPY_ERROR_MEMORY;
    ve_array *w = where;
    double init_val = (double)(*pi);

    // initialize
    if (init_flag) {
@#ifdef _OPENMP
@#pragma omp for
@#endif /* _OPENMP */
        for (uint64_t i=0; i<y->size; i++) py[i] = init_val;
    }
    else {
@#ifdef _OPENMP
@#pragma omp for
@#endif /* _OPENMP */
        for (uint64_t i=0; i<y->size; i++) py[i] = 0;
    }

/////////
// 0-d //
/////////
    if (x->ndim == 0){
@#ifdef _OPENMP
@#pragma omp single
@#endif /* _OPENMP */
{
        if (!where_flag) {
            if (!init_flag) {
                *py = *px;
            }
            else{
                @UNARY_OPERATOR@(*px,*py,f64)
            }
        } else {
            Bint *pw = (Bint *)w->ve_adr;
            if (*pw) {
                if (!init_flag) {
                    *py = *px;
                }
                else{
                    @UNARY_OPERATOR@(*px,*py,f64)
                }
            }
        }
} /* omp single */

/////////
// 1-d //
/////////
    } else if (x->ndim == 1){
        uint64_t i;
        uint64_t iw0 = 0;
        const uint64_t ix = x->strides[0]/x->itemsize;
        if (!where_flag) {
#if defined(add_reduce) || defined(multiply_reduce)
            double tmp;
#if defined(add_reduce)
            nlcpy__global_f64 = 0;
@#pragma omp barrier
            tmp = 0;
#elif defined(multiply_reduce)
            nlcpy__global_f64 = 1;
@#pragma omp barrier
            tmp = 1;
#else
#error add_reduce or minimum_reduce must be defined.
#endif
            uint64_t ii;
            if (!init_flag) {
                *py = *px;
                ii = 1;
            } else {
                ii = 0;
            }
            const int it = omp_get_thread_num();
            const int nt = omp_get_max_threads();
            const uint64_t ist = (x->shape[0]-ii)*it/nt;
            const uint64_t ien = (x->shape[0]-ii)*(it+1)/nt;
            if (it==0) tmp = (double)(*py);
@#pragma _NEC ivdep
            for (i=ii+ist; i < ii+ien; i++) {
                @UNARY_OPERATOR@(px[i*ix],tmp,f64)
            }
@#pragma _NEC novector
            for (i=0; i < nt; i++) {
                if (i==it) {
                    @UNARY_OPERATOR@(tmp,nlcpy__global_f64,f64)
                }
@#pragma omp barrier
            }
            *py = nlcpy__global_f64;
#elif ( defined(maximum_reduce) || defined(minimum_reduce) ) && ( defined(DTAG1_i32) || defined(DTAG1_i64) || defined(DTAG1_f32) || defined(DTAG1_f64) || defined(DTAG1_bool)) && ( "f64" eq "i32" || "f64" eq "i64" || "f64" eq "f32" || "f64" eq "f64" || "f64" eq "bool")
            // Note that rational operations for complex and unsigned numbers occurs an error or warnning.
            uint64_t ii;
            if (!init_flag) {
                *py = *px;
                ii = 1;
            } else {
                ii = 0;
            }
            nlcpy__global_f64 = (double)(*py);
@#pragma omp barrier
            const int it = omp_get_thread_num();
            const int nt = omp_get_max_threads();
            const uint64_t ist = (x->shape[0]-ii)*it/nt;
            const uint64_t ien = (x->shape[0]-ii)*(it+1)/nt;
            double tmp = (double)(*py);
#if "f64" eq "f32" || "f64" eq "f64"
            double is_there_nan = (isnan_f64(tmp)) ? 1.0 : 0.0;
#elif "f64" eq "i32" || "f64" eq "i64" || "f64" eq "bool"
            double is_there_nan = 0.0;
#else
#error Not Impletended
#endif
@#pragma _NEC ivdep
            for (i=ii+ist; i < ii+ien; i++) {
#if defined(maximum_reduce)
                tmp = (tmp > px[i*ix]) ? tmp : (double)px[i*ix];
#elif defined(minimum_reduce)
                tmp = (tmp < px[i*ix]) ? tmp : (double)px[i*ix];
#endif
#if defined(DTAG1_f32) || defined(DTAG1_f64)
                // NaN check:
                // is_there_nan += (float)isnan_@DTAG1@(px[i*ix]);
                @TYPE1@ xx;
                *(&xx) = px[i*ix];
                // The following line checks for NaN and it is in an unnatural way.
                // Here, NaN == NaN becomes False.
                // If there is one or more qNaN, signaling NaN (sNaN) occurs.
                // qNaN might not be detected qNaN by compiler optimizations.
                // However, we prioritize the performance.
                is_there_nan += (! (xx == px[i*ix]) ) ? 1.0 : 0.0;
#endif
            }
#if defined(DTAG1_f32) || defined(DTAG1_f64)
            if (is_there_nan != (float)0) {
                tmp = NAN;
                // In the following function call, PSW flags are manipulated to skip the intentional sNaN above.
                retrieve_fpe_flags(psw);
                *psw &= 0x0000003d;
                set_fpe_flags(*psw);
            }
#endif
@#pragma omp critical
            {
                @UNARY_OPERATOR@(tmp,nlcpy__global_f64,f64)
            }
@#pragma omp barrier
            *py = nlcpy__global_f64;
#else
        // Unvectorizable case
@#ifdef _OPENMP
@#pragma omp single
@#endif /* _OPENMP */
            {
                if (!init_flag) {
                    *py = *px;
                    i = 1;
                } else {
                    i = 0;
                }
@#pragma _NEC novector
                for (; i < x->shape[0]; i++) {
                    @UNARY_OPERATOR@(px[i*ix],*py,f64)
                }
            } /* omp single */
#endif
        } else {
@#ifdef _OPENMP
@#pragma omp single
@#endif /* _OPENMP */
            {
                Bint *pw = (Bint *)w->ve_adr;
                const uint64_t iw = w->strides[0]/w->itemsize;
                if (!init_flag) {
                    for (i = 0; i < x->shape[0]; i++) {
                        if (pw[i*iw]) {
                            *py = *px;
                            i++;
                            break;
                        }
                    }
                } else {
                    i = 0;
                }
#ifdef left_shift_reduce
@#pragma _NEC novector
#else
@#pragma _NEC ivdep
#endif
                for (; i < x->shape[0]; i++) {
                    if (pw[i*iw]) {
                        @UNARY_OPERATOR@(px[i*ix],*py,f64)
                    }
                }
            } /* omp single */
        }

/////////
// N-d //
/////////
    } else if (x->ndim <= NLCPY_MAXNDIM) {
        int64_t n_inner = x->ndim - 1;
        int64_t n_outer = 0;

        int64_t *idx = (int64_t*)alloca(sizeof(int64_t)*x->ndim);
        for (uint64_t i = 0; i < x->ndim; i++) {
            idx[i] = i;
        }
        if (x->ndim > 2) {
            int64_t max_idx[3], tmp;
            nlcpy__argnsort(x, max_idx, 3);
#ifndef add_reduce
            if (idx[max_idx[0]] == axis) {
                max_idx[0] = max_idx[1];
            }
#endif
            if (max_idx[0] != n_inner) {
                tmp = idx[n_inner];
                idx[n_inner] = idx[max_idx[0]];
                idx[max_idx[0]] = tmp;
            }
            if (idx[max_idx[1]] == axis) {
                max_idx[1] = max_idx[2];
            }
            if (idx[max_idx[1]] != n_outer) {
                tmp = idx[n_outer];
                idx[n_outer] = idx[max_idx[1]];
                idx[max_idx[1]] = tmp;
            }
        } else if (n_outer == axis) {
            idx[0] = 1;
            idx[1] = 0;
        }
        int64_t n_inner2 = idx[n_inner];
        int64_t n_outer2 = idx[n_outer];

@#ifdef _OPENMP
        const int nt = omp_get_num_threads();
        const int it = omp_get_thread_num();
@#else
        const int nt = 1;
        const int it = 0;
@#endif
        int64_t i;
        uint64_t adr_x = (uint64_t)x->ve_adr;
        uint64_t adr_y = (uint64_t)y->ve_adr;

        uint64_t ix = 0;
        uint64_t iy = 0;
        uint64_t iw = 0;
        uint64_t ix0 = x->strides[n_inner2] / x->itemsize;
        uint64_t iy0 = y->strides[n_inner2] / y->itemsize;
        uint64_t iw0 = 0;
        int64_t k = 0;
        int64_t *cnt_x = (int64_t*)alloca(sizeof(int64_t)*x->ndim);
        nlcpy__reset_coords(cnt_x, x->ndim);
        const int64_t lenm = x->shape[n_outer2];
        const int64_t cntm_s = lenm * it / nt;
        const int64_t cntm_e = lenm * (it + 1) / nt;
        if (!where_flag) {
            for (int64_t cntm = cntm_s; cntm < cntm_e; cntm++) {
                cnt_x[n_outer2] = cntm;
                ix = cntm * x->strides[n_outer2] / x->itemsize;
                iy = (n_outer2 == axis) ? 0 : cntm * y->strides[n_outer2] / y->itemsize;

                do {
                    if (n_inner2 == axis) {
                        // Note that a rational operation for complex and unsigned numbers occurs an error or warnning.
                        uint64_t ii;
                        if (!init_flag) {
                           py[iy] = px[ix];
                           ii = 1;
                        } else {
                           ii = 0;
                        }
#if ( defined(maximum_reduce) || defined(minimum_reduce) ) && ( defined(DTAG1_i32) || defined(DTAG1_i64) || defined(DTAG1_f32) || defined(DTAG1_f64) || defined(DTAG1_bool)) && ( "f64" eq "i32" || "f64" eq "i64" || "f64" eq "f32" || "f64" eq "f64" || "f64" eq "bool")
                        double tmp = py[iy];
#if "f64" eq "f32" || "f64" eq "f64"
                        float is_there_nan = (isnan_f64(py[iy])) ? 1.0 : 0.0;
#elif "f64" eq "i32" || "f64" eq "i64" || "f64" eq "bool"
                        float is_there_nan = 0.0;
#else
#error Not Impletended
#endif
@#pragma _NEC ivdep
                        for (i = ii; i < x->shape[n_inner2]; i++) {
#if defined(maximum_reduce)
                            tmp = (tmp > px[i*ix0+ix]) ? tmp : (double)px[i*ix0+ix];
#elif defined(minimum_reduce)
                            tmp = (tmp < px[i*ix0+ix]) ? tmp : (double)px[i*ix0+ix];
#endif
#if defined(DTAG1_f32) || defined(DTAG1_f64)
                            // NaN check:
                            //   is_there_nan += (float)isnan_@DTAG1@(px[i*ix0+ix]);
                            @TYPE1@ xx;
                            *(&xx) = px[i*ix0+ix];
                            // The following line checks for NaN and it is in an unnatural way.
                            // Here, NaN == NaN becomes False.
                            // If there is one or more qNaN, signaling NaN (sNaN) occurs.
                            // qNaN might not be detected qNaN by compiler optimizations.
                            // However, we prioritize the performance.
                            is_there_nan += (! (xx == px[i*ix0+ix]) ) ? 1.0 : 0.0;
#endif
                        }
#if defined(DTAG1_f32) || defined(DTAG1_f64)
                        if (is_there_nan != (float)0) {
                            tmp = NAN;
                            // In the following function call, PSW flags are manipulated to skip the intentional sNaN above.
                            retrieve_fpe_flags(psw);
                            *psw &= 0x0000003d;
                            set_fpe_flags(*psw);
                        }
#endif
                        py[iy] = tmp;
#else
                        // General case
#ifdef left_shift_reduce
@#pragma _NEC novector
#else
@#pragma _NEC ivdep
#endif
                        for (i = ii; i < x->shape[n_inner2]; i++) {
                            @UNARY_OPERATOR@(px[i*ix0+ix],py[iy],f64)
                        }
#endif
                    } else {
                        if (init_flag || cnt_x[axis] > 0) {
#ifdef left_shift_reduce
@#pragma _NEC novector
#else
@#pragma _NEC ivdep
#endif
                            for (i = 0; i < x->shape[n_inner2]; i++) {
                                @UNARY_OPERATOR@(px[i*ix0+ix],py[i*iy0+iy],f64)
                             }
                        } else {
                            for (i = 0; i < x->shape[n_inner2]; i++) {
                                py[i*iy0+iy] = @CAST_OPERATOR@(px[i*ix0+ix],@DTAG1@,f64)
                            }
                        }
                    }
                    for (k = n_inner - 1; k >= 1; k--) {
                        int64_t kk = idx[k];
                        if (kk == axis) {
                            if (++cnt_x[kk] < x->shape[kk]) {
                                ix += x->strides[kk] / x->itemsize;
                                break;
                            }
                            ix -= (x->strides[kk] / x->itemsize) * (x->shape[kk] - 1);
                        } else {
                            if (++cnt_x[kk] < x->shape[kk]) {
                                ix += x->strides[kk] / x->itemsize;
                                iy += y->strides[kk] / y->itemsize;
                                break;
                            }
                            ix -= (x->strides[kk] / x->itemsize) * (x->shape[kk] - 1);
                            iy -= (y->strides[kk] / y->itemsize) * (y->shape[kk] - 1);
                        }
                        cnt_x[kk] = 0;
                    }
                } while (k >= 1);
            }
        } else {
            Bint *pw = (Bint *)w->ve_adr;
            iw0 = w->strides[n_inner2] / w->itemsize;

            for (int64_t cntm = cntm_s; cntm < cntm_e; cntm++) {
                ix = cntm * x->strides[n_outer2] / x->itemsize;
                iy = (n_outer2 == axis) ? 0 : cntm * y->strides[n_outer2] / y->itemsize;
                iw = cntm * w->strides[n_outer2] / w->itemsize;

                do {
                    if (n_inner2 == axis) {
                        if (!init_flag) {
                            for (i = 0; i < w->shape[n_inner2]; i++) {
                                if (pw[i*iw0+iw]) {
                                    py[iy] = @CAST_OPERATOR@(px[i*ix0+ix],@DTAG1@,f64)
                                    i++;
                                    break;
                                }
                            }
                        } else {
                            i = 0;
                        }
#ifdef left_shift_reduce
@#pragma _NEC novector
#else
@#pragma _NEC ivdep
#endif
                        for (; i < x->shape[n_inner2]; i++) {
                            if (pw[i*iw0+iw]) {
                                @UNARY_OPERATOR@(px[i*ix0+ix],py[iy],f64)
                            }
                        }
                    } else {
                        if (init_flag || cnt_x[axis] > 0) {
#ifdef left_shift_reduce
@#pragma _NEC novector
#else
@#pragma _NEC ivdep
#endif
                            for (i = 0; i < x->shape[n_inner2]; i++) {
                                if (pw[i*iw0+iw]) {
                                    @UNARY_OPERATOR@(px[i*ix0+ix],py[i*iy0+iy],f64)
                                }
                            }
                        } else {
                            for (i = 0; i < x->shape[n_inner2]; i++) {
                                if (pw[i*iw0+iw]) {
                                    py[i*iy0+iy] = @CAST_OPERATOR@(px[i*ix0+ix],@DTAG1@,f64)
                                }
                            }
                        }
                    }
                    for (k = n_inner - 1; k >= 1; k--) {
                        int64_t kk = idx[k];
                        if (kk == axis) {
                            if (++cnt_x[kk] < x->shape[kk]) {
                                ix += x->strides[kk] / x->itemsize;
                                iw += w->strides[kk] / w->itemsize;
                                break;
                            }
                            ix -= (x->strides[kk] / x->itemsize) * (x->shape[kk] - 1);
                            iw -= (w->strides[kk] / w->itemsize) * (x->shape[kk] - 1);
                        } else {
                            if (++cnt_x[kk] < x->shape[kk]) {
                                ix += x->strides[kk] / x->itemsize;
                                iy += y->strides[kk] / y->itemsize;
                                iw += w->strides[kk] / w->itemsize;
                                break;
                            }
                            ix -= (x->strides[kk] / x->itemsize) * (x->shape[kk] - 1);
                            iy -= (y->strides[kk] / y->itemsize) * (x->shape[kk] - 1);
                            iw -= (w->strides[kk] / w->itemsize) * (x->shape[kk] - 1);
                        }
                        cnt_x[kk] = 0;
                    }
                } while (k >= 1);
            }
        }
    } else {
        return (uint64_t)NLCPY_ERROR_NDIM;
    }
    retrieve_fpe_flags(psw);
    return (uint64_t)NLCPY_ERROR_OK;
}


#endif
#if defined(DTAG_c64)

uint64_t FILENAME_@DTAG1@_c64(ve_array *x, ve_array *y, int32_t axis, int32_t init_flag,
                     ve_array *initial, int32_t where_flag, ve_array *where, int32_t *psw)
{
    @TYPE1@ *px = (@TYPE1@ *)nlcpy__get_ptr(x);
    if (px == NULL) return NLCPY_ERROR_MEMORY;
    float _Complex *py = (float _Complex *)nlcpy__get_ptr(y);
    if (py == NULL) return NLCPY_ERROR_MEMORY;
    float _Complex *pi = (float _Complex *)nlcpy__get_ptr(initial);
    if (pi == NULL) return NLCPY_ERROR_MEMORY;
    ve_array *w = where;
    float _Complex init_val = (float _Complex)(*pi);

    // initialize
    if (init_flag) {
@#ifdef _OPENMP
@#pragma omp for
@#endif /* _OPENMP */
        for (uint64_t i=0; i<y->size; i++) py[i] = init_val;
    }
    else {
@#ifdef _OPENMP
@#pragma omp for
@#endif /* _OPENMP */
        for (uint64_t i=0; i<y->size; i++) py[i] = 0;
    }

/////////
// 0-d //
/////////
    if (x->ndim == 0){
@#ifdef _OPENMP
@#pragma omp single
@#endif /* _OPENMP */
{
        if (!where_flag) {
            if (!init_flag) {
                *py = *px;
            }
            else{
                @UNARY_OPERATOR@(*px,*py,c64)
            }
        } else {
            Bint *pw = (Bint *)w->ve_adr;
            if (*pw) {
                if (!init_flag) {
                    *py = *px;
                }
                else{
                    @UNARY_OPERATOR@(*px,*py,c64)
                }
            }
        }
} /* omp single */

/////////
// 1-d //
/////////
    } else if (x->ndim == 1){
        uint64_t i;
        uint64_t iw0 = 0;
        const uint64_t ix = x->strides[0]/x->itemsize;
        if (!where_flag) {
#if defined(add_reduce) || defined(multiply_reduce)
            float _Complex tmp;
#if defined(add_reduce)
            nlcpy__global_c64 = 0;
@#pragma omp barrier
            tmp = 0;
#elif defined(multiply_reduce)
            nlcpy__global_c64 = 1;
@#pragma omp barrier
            tmp = 1;
#else
#error add_reduce or minimum_reduce must be defined.
#endif
            uint64_t ii;
            if (!init_flag) {
                *py = *px;
                ii = 1;
            } else {
                ii = 0;
            }
            const int it = omp_get_thread_num();
            const int nt = omp_get_max_threads();
            const uint64_t ist = (x->shape[0]-ii)*it/nt;
            const uint64_t ien = (x->shape[0]-ii)*(it+1)/nt;
            if (it==0) tmp = (float _Complex)(*py);
@#pragma _NEC ivdep
            for (i=ii+ist; i < ii+ien; i++) {
                @UNARY_OPERATOR@(px[i*ix],tmp,c64)
            }
@#pragma _NEC novector
            for (i=0; i < nt; i++) {
                if (i==it) {
                    @UNARY_OPERATOR@(tmp,nlcpy__global_c64,c64)
                }
@#pragma omp barrier
            }
            *py = nlcpy__global_c64;
#elif ( defined(maximum_reduce) || defined(minimum_reduce) ) && ( defined(DTAG1_i32) || defined(DTAG1_i64) || defined(DTAG1_f32) || defined(DTAG1_f64) || defined(DTAG1_bool)) && ( "c64" eq "i32" || "c64" eq "i64" || "c64" eq "f32" || "c64" eq "f64" || "c64" eq "bool")
            // Note that rational operations for complex and unsigned numbers occurs an error or warnning.
            uint64_t ii;
            if (!init_flag) {
                *py = *px;
                ii = 1;
            } else {
                ii = 0;
            }
            nlcpy__global_c64 = (float _Complex)(*py);
@#pragma omp barrier
            const int it = omp_get_thread_num();
            const int nt = omp_get_max_threads();
            const uint64_t ist = (x->shape[0]-ii)*it/nt;
            const uint64_t ien = (x->shape[0]-ii)*(it+1)/nt;
            float _Complex tmp = (float _Complex)(*py);
#if "c64" eq "f32" || "c64" eq "f64"
            double is_there_nan = (isnan_c64(tmp)) ? 1.0 : 0.0;
#elif "c64" eq "i32" || "c64" eq "i64" || "c64" eq "bool"
            double is_there_nan = 0.0;
#else
#error Not Impletended
#endif
@#pragma _NEC ivdep
            for (i=ii+ist; i < ii+ien; i++) {
#if defined(maximum_reduce)
                tmp = (tmp > px[i*ix]) ? tmp : (float _Complex)px[i*ix];
#elif defined(minimum_reduce)
                tmp = (tmp < px[i*ix]) ? tmp : (float _Complex)px[i*ix];
#endif
#if defined(DTAG1_f32) || defined(DTAG1_f64)
                // NaN check:
                // is_there_nan += (float)isnan_@DTAG1@(px[i*ix]);
                @TYPE1@ xx;
                *(&xx) = px[i*ix];
                // The following line checks for NaN and it is in an unnatural way.
                // Here, NaN == NaN becomes False.
                // If there is one or more qNaN, signaling NaN (sNaN) occurs.
                // qNaN might not be detected qNaN by compiler optimizations.
                // However, we prioritize the performance.
                is_there_nan += (! (xx == px[i*ix]) ) ? 1.0 : 0.0;
#endif
            }
#if defined(DTAG1_f32) || defined(DTAG1_f64)
            if (is_there_nan != (float)0) {
                tmp = NAN;
                // In the following function call, PSW flags are manipulated to skip the intentional sNaN above.
                retrieve_fpe_flags(psw);
                *psw &= 0x0000003d;
                set_fpe_flags(*psw);
            }
#endif
@#pragma omp critical
            {
                @UNARY_OPERATOR@(tmp,nlcpy__global_c64,c64)
            }
@#pragma omp barrier
            *py = nlcpy__global_c64;
#else
        // Unvectorizable case
@#ifdef _OPENMP
@#pragma omp single
@#endif /* _OPENMP */
            {
                if (!init_flag) {
                    *py = *px;
                    i = 1;
                } else {
                    i = 0;
                }
@#pragma _NEC novector
                for (; i < x->shape[0]; i++) {
                    @UNARY_OPERATOR@(px[i*ix],*py,c64)
                }
            } /* omp single */
#endif
        } else {
@#ifdef _OPENMP
@#pragma omp single
@#endif /* _OPENMP */
            {
                Bint *pw = (Bint *)w->ve_adr;
                const uint64_t iw = w->strides[0]/w->itemsize;
                if (!init_flag) {
                    for (i = 0; i < x->shape[0]; i++) {
                        if (pw[i*iw]) {
                            *py = *px;
                            i++;
                            break;
                        }
                    }
                } else {
                    i = 0;
                }
#ifdef left_shift_reduce
@#pragma _NEC novector
#else
@#pragma _NEC ivdep
#endif
                for (; i < x->shape[0]; i++) {
                    if (pw[i*iw]) {
                        @UNARY_OPERATOR@(px[i*ix],*py,c64)
                    }
                }
            } /* omp single */
        }

/////////
// N-d //
/////////
    } else if (x->ndim <= NLCPY_MAXNDIM) {
        int64_t n_inner = x->ndim - 1;
        int64_t n_outer = 0;

        int64_t *idx = (int64_t*)alloca(sizeof(int64_t)*x->ndim);
        for (uint64_t i = 0; i < x->ndim; i++) {
            idx[i] = i;
        }
        if (x->ndim > 2) {
            int64_t max_idx[3], tmp;
            nlcpy__argnsort(x, max_idx, 3);
#ifndef add_reduce
            if (idx[max_idx[0]] == axis) {
                max_idx[0] = max_idx[1];
            }
#endif
            if (max_idx[0] != n_inner) {
                tmp = idx[n_inner];
                idx[n_inner] = idx[max_idx[0]];
                idx[max_idx[0]] = tmp;
            }
            if (idx[max_idx[1]] == axis) {
                max_idx[1] = max_idx[2];
            }
            if (idx[max_idx[1]] != n_outer) {
                tmp = idx[n_outer];
                idx[n_outer] = idx[max_idx[1]];
                idx[max_idx[1]] = tmp;
            }
        } else if (n_outer == axis) {
            idx[0] = 1;
            idx[1] = 0;
        }
        int64_t n_inner2 = idx[n_inner];
        int64_t n_outer2 = idx[n_outer];

@#ifdef _OPENMP
        const int nt = omp_get_num_threads();
        const int it = omp_get_thread_num();
@#else
        const int nt = 1;
        const int it = 0;
@#endif
        int64_t i;
        uint64_t adr_x = (uint64_t)x->ve_adr;
        uint64_t adr_y = (uint64_t)y->ve_adr;

        uint64_t ix = 0;
        uint64_t iy = 0;
        uint64_t iw = 0;
        uint64_t ix0 = x->strides[n_inner2] / x->itemsize;
        uint64_t iy0 = y->strides[n_inner2] / y->itemsize;
        uint64_t iw0 = 0;
        int64_t k = 0;
        int64_t *cnt_x = (int64_t*)alloca(sizeof(int64_t)*x->ndim);
        nlcpy__reset_coords(cnt_x, x->ndim);
        const int64_t lenm = x->shape[n_outer2];
        const int64_t cntm_s = lenm * it / nt;
        const int64_t cntm_e = lenm * (it + 1) / nt;
        if (!where_flag) {
            for (int64_t cntm = cntm_s; cntm < cntm_e; cntm++) {
                cnt_x[n_outer2] = cntm;
                ix = cntm * x->strides[n_outer2] / x->itemsize;
                iy = (n_outer2 == axis) ? 0 : cntm * y->strides[n_outer2] / y->itemsize;

                do {
                    if (n_inner2 == axis) {
                        // Note that a rational operation for complex and unsigned numbers occurs an error or warnning.
                        uint64_t ii;
                        if (!init_flag) {
                           py[iy] = px[ix];
                           ii = 1;
                        } else {
                           ii = 0;
                        }
#if ( defined(maximum_reduce) || defined(minimum_reduce) ) && ( defined(DTAG1_i32) || defined(DTAG1_i64) || defined(DTAG1_f32) || defined(DTAG1_f64) || defined(DTAG1_bool)) && ( "c64" eq "i32" || "c64" eq "i64" || "c64" eq "f32" || "c64" eq "f64" || "c64" eq "bool")
                        float _Complex tmp = py[iy];
#if "c64" eq "f32" || "c64" eq "f64"
                        float is_there_nan = (isnan_c64(py[iy])) ? 1.0 : 0.0;
#elif "c64" eq "i32" || "c64" eq "i64" || "c64" eq "bool"
                        float is_there_nan = 0.0;
#else
#error Not Impletended
#endif
@#pragma _NEC ivdep
                        for (i = ii; i < x->shape[n_inner2]; i++) {
#if defined(maximum_reduce)
                            tmp = (tmp > px[i*ix0+ix]) ? tmp : (float _Complex)px[i*ix0+ix];
#elif defined(minimum_reduce)
                            tmp = (tmp < px[i*ix0+ix]) ? tmp : (float _Complex)px[i*ix0+ix];
#endif
#if defined(DTAG1_f32) || defined(DTAG1_f64)
                            // NaN check:
                            //   is_there_nan += (float)isnan_@DTAG1@(px[i*ix0+ix]);
                            @TYPE1@ xx;
                            *(&xx) = px[i*ix0+ix];
                            // The following line checks for NaN and it is in an unnatural way.
                            // Here, NaN == NaN becomes False.
                            // If there is one or more qNaN, signaling NaN (sNaN) occurs.
                            // qNaN might not be detected qNaN by compiler optimizations.
                            // However, we prioritize the performance.
                            is_there_nan += (! (xx == px[i*ix0+ix]) ) ? 1.0 : 0.0;
#endif
                        }
#if defined(DTAG1_f32) || defined(DTAG1_f64)
                        if (is_there_nan != (float)0) {
                            tmp = NAN;
                            // In the following function call, PSW flags are manipulated to skip the intentional sNaN above.
                            retrieve_fpe_flags(psw);
                            *psw &= 0x0000003d;
                            set_fpe_flags(*psw);
                        }
#endif
                        py[iy] = tmp;
#else
                        // General case
#ifdef left_shift_reduce
@#pragma _NEC novector
#else
@#pragma _NEC ivdep
#endif
                        for (i = ii; i < x->shape[n_inner2]; i++) {
                            @UNARY_OPERATOR@(px[i*ix0+ix],py[iy],c64)
                        }
#endif
                    } else {
                        if (init_flag || cnt_x[axis] > 0) {
#ifdef left_shift_reduce
@#pragma _NEC novector
#else
@#pragma _NEC ivdep
#endif
                            for (i = 0; i < x->shape[n_inner2]; i++) {
                                @UNARY_OPERATOR@(px[i*ix0+ix],py[i*iy0+iy],c64)
                             }
                        } else {
                            for (i = 0; i < x->shape[n_inner2]; i++) {
                                py[i*iy0+iy] = @CAST_OPERATOR@(px[i*ix0+ix],@DTAG1@,c64)
                            }
                        }
                    }
                    for (k = n_inner - 1; k >= 1; k--) {
                        int64_t kk = idx[k];
                        if (kk == axis) {
                            if (++cnt_x[kk] < x->shape[kk]) {
                                ix += x->strides[kk] / x->itemsize;
                                break;
                            }
                            ix -= (x->strides[kk] / x->itemsize) * (x->shape[kk] - 1);
                        } else {
                            if (++cnt_x[kk] < x->shape[kk]) {
                                ix += x->strides[kk] / x->itemsize;
                                iy += y->strides[kk] / y->itemsize;
                                break;
                            }
                            ix -= (x->strides[kk] / x->itemsize) * (x->shape[kk] - 1);
                            iy -= (y->strides[kk] / y->itemsize) * (y->shape[kk] - 1);
                        }
                        cnt_x[kk] = 0;
                    }
                } while (k >= 1);
            }
        } else {
            Bint *pw = (Bint *)w->ve_adr;
            iw0 = w->strides[n_inner2] / w->itemsize;

            for (int64_t cntm = cntm_s; cntm < cntm_e; cntm++) {
                ix = cntm * x->strides[n_outer2] / x->itemsize;
                iy = (n_outer2 == axis) ? 0 : cntm * y->strides[n_outer2] / y->itemsize;
                iw = cntm * w->strides[n_outer2] / w->itemsize;

                do {
                    if (n_inner2 == axis) {
                        if (!init_flag) {
                            for (i = 0; i < w->shape[n_inner2]; i++) {
                                if (pw[i*iw0+iw]) {
                                    py[iy] = @CAST_OPERATOR@(px[i*ix0+ix],@DTAG1@,c64)
                                    i++;
                                    break;
                                }
                            }
                        } else {
                            i = 0;
                        }
#ifdef left_shift_reduce
@#pragma _NEC novector
#else
@#pragma _NEC ivdep
#endif
                        for (; i < x->shape[n_inner2]; i++) {
                            if (pw[i*iw0+iw]) {
                                @UNARY_OPERATOR@(px[i*ix0+ix],py[iy],c64)
                            }
                        }
                    } else {
                        if (init_flag || cnt_x[axis] > 0) {
#ifdef left_shift_reduce
@#pragma _NEC novector
#else
@#pragma _NEC ivdep
#endif
                            for (i = 0; i < x->shape[n_inner2]; i++) {
                                if (pw[i*iw0+iw]) {
                                    @UNARY_OPERATOR@(px[i*ix0+ix],py[i*iy0+iy],c64)
                                }
                            }
                        } else {
                            for (i = 0; i < x->shape[n_inner2]; i++) {
                                if (pw[i*iw0+iw]) {
                                    py[i*iy0+iy] = @CAST_OPERATOR@(px[i*ix0+ix],@DTAG1@,c64)
                                }
                            }
                        }
                    }
                    for (k = n_inner - 1; k >= 1; k--) {
                        int64_t kk = idx[k];
                        if (kk == axis) {
                            if (++cnt_x[kk] < x->shape[kk]) {
                                ix += x->strides[kk] / x->itemsize;
                                iw += w->strides[kk] / w->itemsize;
                                break;
                            }
                            ix -= (x->strides[kk] / x->itemsize) * (x->shape[kk] - 1);
                            iw -= (w->strides[kk] / w->itemsize) * (x->shape[kk] - 1);
                        } else {
                            if (++cnt_x[kk] < x->shape[kk]) {
                                ix += x->strides[kk] / x->itemsize;
                                iy += y->strides[kk] / y->itemsize;
                                iw += w->strides[kk] / w->itemsize;
                                break;
                            }
                            ix -= (x->strides[kk] / x->itemsize) * (x->shape[kk] - 1);
                            iy -= (y->strides[kk] / y->itemsize) * (x->shape[kk] - 1);
                            iw -= (w->strides[kk] / w->itemsize) * (x->shape[kk] - 1);
                        }
                        cnt_x[kk] = 0;
                    }
                } while (k >= 1);
            }
        }
    } else {
        return (uint64_t)NLCPY_ERROR_NDIM;
    }
    retrieve_fpe_flags(psw);
    return (uint64_t)NLCPY_ERROR_OK;
}


#endif
#if defined(DTAG_c128)

uint64_t FILENAME_@DTAG1@_c128(ve_array *x, ve_array *y, int32_t axis, int32_t init_flag,
                     ve_array *initial, int32_t where_flag, ve_array *where, int32_t *psw)
{
    @TYPE1@ *px = (@TYPE1@ *)nlcpy__get_ptr(x);
    if (px == NULL) return NLCPY_ERROR_MEMORY;
    double _Complex *py = (double _Complex *)nlcpy__get_ptr(y);
    if (py == NULL) return NLCPY_ERROR_MEMORY;
    double _Complex *pi = (double _Complex *)nlcpy__get_ptr(initial);
    if (pi == NULL) return NLCPY_ERROR_MEMORY;
    ve_array *w = where;
    double _Complex init_val = (double _Complex)(*pi);

    // initialize
    if (init_flag) {
@#ifdef _OPENMP
@#pragma omp for
@#endif /* _OPENMP */
        for (uint64_t i=0; i<y->size; i++) py[i] = init_val;
    }
    else {
@#ifdef _OPENMP
@#pragma omp for
@#endif /* _OPENMP */
        for (uint64_t i=0; i<y->size; i++) py[i] = 0;
    }

/////////
// 0-d //
/////////
    if (x->ndim == 0){
@#ifdef _OPENMP
@#pragma omp single
@#endif /* _OPENMP */
{
        if (!where_flag) {
            if (!init_flag) {
                *py = *px;
            }
            else{
                @UNARY_OPERATOR@(*px,*py,c128)
            }
        } else {
            Bint *pw = (Bint *)w->ve_adr;
            if (*pw) {
                if (!init_flag) {
                    *py = *px;
                }
                else{
                    @UNARY_OPERATOR@(*px,*py,c128)
                }
            }
        }
} /* omp single */

/////////
// 1-d //
/////////
    } else if (x->ndim == 1){
        uint64_t i;
        uint64_t iw0 = 0;
        const uint64_t ix = x->strides[0]/x->itemsize;
        if (!where_flag) {
#if defined(add_reduce) || defined(multiply_reduce)
            double _Complex tmp;
#if defined(add_reduce)
            nlcpy__global_c128 = 0;
@#pragma omp barrier
            tmp = 0;
#elif defined(multiply_reduce)
            nlcpy__global_c128 = 1;
@#pragma omp barrier
            tmp = 1;
#else
#error add_reduce or minimum_reduce must be defined.
#endif
            uint64_t ii;
            if (!init_flag) {
                *py = *px;
                ii = 1;
            } else {
                ii = 0;
            }
            const int it = omp_get_thread_num();
            const int nt = omp_get_max_threads();
            const uint64_t ist = (x->shape[0]-ii)*it/nt;
            const uint64_t ien = (x->shape[0]-ii)*(it+1)/nt;
            if (it==0) tmp = (double _Complex)(*py);
@#pragma _NEC ivdep
            for (i=ii+ist; i < ii+ien; i++) {
                @UNARY_OPERATOR@(px[i*ix],tmp,c128)
            }
@#pragma _NEC novector
            for (i=0; i < nt; i++) {
                if (i==it) {
                    @UNARY_OPERATOR@(tmp,nlcpy__global_c128,c128)
                }
@#pragma omp barrier
            }
            *py = nlcpy__global_c128;
#elif ( defined(maximum_reduce) || defined(minimum_reduce) ) && ( defined(DTAG1_i32) || defined(DTAG1_i64) || defined(DTAG1_f32) || defined(DTAG1_f64) || defined(DTAG1_bool)) && ( "c128" eq "i32" || "c128" eq "i64" || "c128" eq "f32" || "c128" eq "f64" || "c128" eq "bool")
            // Note that rational operations for complex and unsigned numbers occurs an error or warnning.
            uint64_t ii;
            if (!init_flag) {
                *py = *px;
                ii = 1;
            } else {
                ii = 0;
            }
            nlcpy__global_c128 = (double _Complex)(*py);
@#pragma omp barrier
            const int it = omp_get_thread_num();
            const int nt = omp_get_max_threads();
            const uint64_t ist = (x->shape[0]-ii)*it/nt;
            const uint64_t ien = (x->shape[0]-ii)*(it+1)/nt;
            double _Complex tmp = (double _Complex)(*py);
#if "c128" eq "f32" || "c128" eq "f64"
            double is_there_nan = (isnan_c128(tmp)) ? 1.0 : 0.0;
#elif "c128" eq "i32" || "c128" eq "i64" || "c128" eq "bool"
            double is_there_nan = 0.0;
#else
#error Not Impletended
#endif
@#pragma _NEC ivdep
            for (i=ii+ist; i < ii+ien; i++) {
#if defined(maximum_reduce)
                tmp = (tmp > px[i*ix]) ? tmp : (double _Complex)px[i*ix];
#elif defined(minimum_reduce)
                tmp = (tmp < px[i*ix]) ? tmp : (double _Complex)px[i*ix];
#endif
#if defined(DTAG1_f32) || defined(DTAG1_f64)
                // NaN check:
                // is_there_nan += (float)isnan_@DTAG1@(px[i*ix]);
                @TYPE1@ xx;
                *(&xx) = px[i*ix];
                // The following line checks for NaN and it is in an unnatural way.
                // Here, NaN == NaN becomes False.
                // If there is one or more qNaN, signaling NaN (sNaN) occurs.
                // qNaN might not be detected qNaN by compiler optimizations.
                // However, we prioritize the performance.
                is_there_nan += (! (xx == px[i*ix]) ) ? 1.0 : 0.0;
#endif
            }
#if defined(DTAG1_f32) || defined(DTAG1_f64)
            if (is_there_nan != (float)0) {
                tmp = NAN;
                // In the following function call, PSW flags are manipulated to skip the intentional sNaN above.
                retrieve_fpe_flags(psw);
                *psw &= 0x0000003d;
                set_fpe_flags(*psw);
            }
#endif
@#pragma omp critical
            {
                @UNARY_OPERATOR@(tmp,nlcpy__global_c128,c128)
            }
@#pragma omp barrier
            *py = nlcpy__global_c128;
#else
        // Unvectorizable case
@#ifdef _OPENMP
@#pragma omp single
@#endif /* _OPENMP */
            {
                if (!init_flag) {
                    *py = *px;
                    i = 1;
                } else {
                    i = 0;
                }
@#pragma _NEC novector
                for (; i < x->shape[0]; i++) {
                    @UNARY_OPERATOR@(px[i*ix],*py,c128)
                }
            } /* omp single */
#endif
        } else {
@#ifdef _OPENMP
@#pragma omp single
@#endif /* _OPENMP */
            {
                Bint *pw = (Bint *)w->ve_adr;
                const uint64_t iw = w->strides[0]/w->itemsize;
                if (!init_flag) {
                    for (i = 0; i < x->shape[0]; i++) {
                        if (pw[i*iw]) {
                            *py = *px;
                            i++;
                            break;
                        }
                    }
                } else {
                    i = 0;
                }
#ifdef left_shift_reduce
@#pragma _NEC novector
#else
@#pragma _NEC ivdep
#endif
                for (; i < x->shape[0]; i++) {
                    if (pw[i*iw]) {
                        @UNARY_OPERATOR@(px[i*ix],*py,c128)
                    }
                }
            } /* omp single */
        }

/////////
// N-d //
/////////
    } else if (x->ndim <= NLCPY_MAXNDIM) {
        int64_t n_inner = x->ndim - 1;
        int64_t n_outer = 0;

        int64_t *idx = (int64_t*)alloca(sizeof(int64_t)*x->ndim);
        for (uint64_t i = 0; i < x->ndim; i++) {
            idx[i] = i;
        }
        if (x->ndim > 2) {
            int64_t max_idx[3], tmp;
            nlcpy__argnsort(x, max_idx, 3);
#ifndef add_reduce
            if (idx[max_idx[0]] == axis) {
                max_idx[0] = max_idx[1];
            }
#endif
            if (max_idx[0] != n_inner) {
                tmp = idx[n_inner];
                idx[n_inner] = idx[max_idx[0]];
                idx[max_idx[0]] = tmp;
            }
            if (idx[max_idx[1]] == axis) {
                max_idx[1] = max_idx[2];
            }
            if (idx[max_idx[1]] != n_outer) {
                tmp = idx[n_outer];
                idx[n_outer] = idx[max_idx[1]];
                idx[max_idx[1]] = tmp;
            }
        } else if (n_outer == axis) {
            idx[0] = 1;
            idx[1] = 0;
        }
        int64_t n_inner2 = idx[n_inner];
        int64_t n_outer2 = idx[n_outer];

@#ifdef _OPENMP
        const int nt = omp_get_num_threads();
        const int it = omp_get_thread_num();
@#else
        const int nt = 1;
        const int it = 0;
@#endif
        int64_t i;
        uint64_t adr_x = (uint64_t)x->ve_adr;
        uint64_t adr_y = (uint64_t)y->ve_adr;

        uint64_t ix = 0;
        uint64_t iy = 0;
        uint64_t iw = 0;
        uint64_t ix0 = x->strides[n_inner2] / x->itemsize;
        uint64_t iy0 = y->strides[n_inner2] / y->itemsize;
        uint64_t iw0 = 0;
        int64_t k = 0;
        int64_t *cnt_x = (int64_t*)alloca(sizeof(int64_t)*x->ndim);
        nlcpy__reset_coords(cnt_x, x->ndim);
        const int64_t lenm = x->shape[n_outer2];
        const int64_t cntm_s = lenm * it / nt;
        const int64_t cntm_e = lenm * (it + 1) / nt;
        if (!where_flag) {
            for (int64_t cntm = cntm_s; cntm < cntm_e; cntm++) {
                cnt_x[n_outer2] = cntm;
                ix = cntm * x->strides[n_outer2] / x->itemsize;
                iy = (n_outer2 == axis) ? 0 : cntm * y->strides[n_outer2] / y->itemsize;

                do {
                    if (n_inner2 == axis) {
                        // Note that a rational operation for complex and unsigned numbers occurs an error or warnning.
                        uint64_t ii;
                        if (!init_flag) {
                           py[iy] = px[ix];
                           ii = 1;
                        } else {
                           ii = 0;
                        }
#if ( defined(maximum_reduce) || defined(minimum_reduce) ) && ( defined(DTAG1_i32) || defined(DTAG1_i64) || defined(DTAG1_f32) || defined(DTAG1_f64) || defined(DTAG1_bool)) && ( "c128" eq "i32" || "c128" eq "i64" || "c128" eq "f32" || "c128" eq "f64" || "c128" eq "bool")
                        double _Complex tmp = py[iy];
#if "c128" eq "f32" || "c128" eq "f64"
                        float is_there_nan = (isnan_c128(py[iy])) ? 1.0 : 0.0;
#elif "c128" eq "i32" || "c128" eq "i64" || "c128" eq "bool"
                        float is_there_nan = 0.0;
#else
#error Not Impletended
#endif
@#pragma _NEC ivdep
                        for (i = ii; i < x->shape[n_inner2]; i++) {
#if defined(maximum_reduce)
                            tmp = (tmp > px[i*ix0+ix]) ? tmp : (double _Complex)px[i*ix0+ix];
#elif defined(minimum_reduce)
                            tmp = (tmp < px[i*ix0+ix]) ? tmp : (double _Complex)px[i*ix0+ix];
#endif
#if defined(DTAG1_f32) || defined(DTAG1_f64)
                            // NaN check:
                            //   is_there_nan += (float)isnan_@DTAG1@(px[i*ix0+ix]);
                            @TYPE1@ xx;
                            *(&xx) = px[i*ix0+ix];
                            // The following line checks for NaN and it is in an unnatural way.
                            // Here, NaN == NaN becomes False.
                            // If there is one or more qNaN, signaling NaN (sNaN) occurs.
                            // qNaN might not be detected qNaN by compiler optimizations.
                            // However, we prioritize the performance.
                            is_there_nan += (! (xx == px[i*ix0+ix]) ) ? 1.0 : 0.0;
#endif
                        }
#if defined(DTAG1_f32) || defined(DTAG1_f64)
                        if (is_there_nan != (float)0) {
                            tmp = NAN;
                            // In the following function call, PSW flags are manipulated to skip the intentional sNaN above.
                            retrieve_fpe_flags(psw);
                            *psw &= 0x0000003d;
                            set_fpe_flags(*psw);
                        }
#endif
                        py[iy] = tmp;
#else
                        // General case
#ifdef left_shift_reduce
@#pragma _NEC novector
#else
@#pragma _NEC ivdep
#endif
                        for (i = ii; i < x->shape[n_inner2]; i++) {
                            @UNARY_OPERATOR@(px[i*ix0+ix],py[iy],c128)
                        }
#endif
                    } else {
                        if (init_flag || cnt_x[axis] > 0) {
#ifdef left_shift_reduce
@#pragma _NEC novector
#else
@#pragma _NEC ivdep
#endif
                            for (i = 0; i < x->shape[n_inner2]; i++) {
                                @UNARY_OPERATOR@(px[i*ix0+ix],py[i*iy0+iy],c128)
                             }
                        } else {
                            for (i = 0; i < x->shape[n_inner2]; i++) {
                                py[i*iy0+iy] = @CAST_OPERATOR@(px[i*ix0+ix],@DTAG1@,c128)
                            }
                        }
                    }
                    for (k = n_inner - 1; k >= 1; k--) {
                        int64_t kk = idx[k];
                        if (kk == axis) {
                            if (++cnt_x[kk] < x->shape[kk]) {
                                ix += x->strides[kk] / x->itemsize;
                                break;
                            }
                            ix -= (x->strides[kk] / x->itemsize) * (x->shape[kk] - 1);
                        } else {
                            if (++cnt_x[kk] < x->shape[kk]) {
                                ix += x->strides[kk] / x->itemsize;
                                iy += y->strides[kk] / y->itemsize;
                                break;
                            }
                            ix -= (x->strides[kk] / x->itemsize) * (x->shape[kk] - 1);
                            iy -= (y->strides[kk] / y->itemsize) * (y->shape[kk] - 1);
                        }
                        cnt_x[kk] = 0;
                    }
                } while (k >= 1);
            }
        } else {
            Bint *pw = (Bint *)w->ve_adr;
            iw0 = w->strides[n_inner2] / w->itemsize;

            for (int64_t cntm = cntm_s; cntm < cntm_e; cntm++) {
                ix = cntm * x->strides[n_outer2] / x->itemsize;
                iy = (n_outer2 == axis) ? 0 : cntm * y->strides[n_outer2] / y->itemsize;
                iw = cntm * w->strides[n_outer2] / w->itemsize;

                do {
                    if (n_inner2 == axis) {
                        if (!init_flag) {
                            for (i = 0; i < w->shape[n_inner2]; i++) {
                                if (pw[i*iw0+iw]) {
                                    py[iy] = @CAST_OPERATOR@(px[i*ix0+ix],@DTAG1@,c128)
                                    i++;
                                    break;
                                }
                            }
                        } else {
                            i = 0;
                        }
#ifdef left_shift_reduce
@#pragma _NEC novector
#else
@#pragma _NEC ivdep
#endif
                        for (; i < x->shape[n_inner2]; i++) {
                            if (pw[i*iw0+iw]) {
                                @UNARY_OPERATOR@(px[i*ix0+ix],py[iy],c128)
                            }
                        }
                    } else {
                        if (init_flag || cnt_x[axis] > 0) {
#ifdef left_shift_reduce
@#pragma _NEC novector
#else
@#pragma _NEC ivdep
#endif
                            for (i = 0; i < x->shape[n_inner2]; i++) {
                                if (pw[i*iw0+iw]) {
                                    @UNARY_OPERATOR@(px[i*ix0+ix],py[i*iy0+iy],c128)
                                }
                            }
                        } else {
                            for (i = 0; i < x->shape[n_inner2]; i++) {
                                if (pw[i*iw0+iw]) {
                                    py[i*iy0+iy] = @CAST_OPERATOR@(px[i*ix0+ix],@DTAG1@,c128)
                                }
                            }
                        }
                    }
                    for (k = n_inner - 1; k >= 1; k--) {
                        int64_t kk = idx[k];
                        if (kk == axis) {
                            if (++cnt_x[kk] < x->shape[kk]) {
                                ix += x->strides[kk] / x->itemsize;
                                iw += w->strides[kk] / w->itemsize;
                                break;
                            }
                            ix -= (x->strides[kk] / x->itemsize) * (x->shape[kk] - 1);
                            iw -= (w->strides[kk] / w->itemsize) * (x->shape[kk] - 1);
                        } else {
                            if (++cnt_x[kk] < x->shape[kk]) {
                                ix += x->strides[kk] / x->itemsize;
                                iy += y->strides[kk] / y->itemsize;
                                iw += w->strides[kk] / w->itemsize;
                                break;
                            }
                            ix -= (x->strides[kk] / x->itemsize) * (x->shape[kk] - 1);
                            iy -= (y->strides[kk] / y->itemsize) * (x->shape[kk] - 1);
                            iw -= (w->strides[kk] / w->itemsize) * (x->shape[kk] - 1);
                        }
                        cnt_x[kk] = 0;
                    }
                } while (k >= 1);
            }
        }
    } else {
        return (uint64_t)NLCPY_ERROR_NDIM;
    }
    retrieve_fpe_flags(psw);
    return (uint64_t)NLCPY_ERROR_OK;
}


#endif
#if defined(DTAG_bool)

uint64_t FILENAME_@DTAG1@_bool(ve_array *x, ve_array *y, int32_t axis, int32_t init_flag,
                     ve_array *initial, int32_t where_flag, ve_array *where, int32_t *psw)
{
    @TYPE1@ *px = (@TYPE1@ *)nlcpy__get_ptr(x);
    if (px == NULL) return NLCPY_ERROR_MEMORY;
    int32_t *py = (int32_t *)nlcpy__get_ptr(y);
    if (py == NULL) return NLCPY_ERROR_MEMORY;
    int32_t *pi = (int32_t *)nlcpy__get_ptr(initial);
    if (pi == NULL) return NLCPY_ERROR_MEMORY;
    ve_array *w = where;
    int32_t init_val = (int32_t)(*pi);

    // initialize
    if (init_flag) {
@#ifdef _OPENMP
@#pragma omp for
@#endif /* _OPENMP */
        for (uint64_t i=0; i<y->size; i++) py[i] = init_val;
    }
    else {
@#ifdef _OPENMP
@#pragma omp for
@#endif /* _OPENMP */
        for (uint64_t i=0; i<y->size; i++) py[i] = 0;
    }

/////////
// 0-d //
/////////
    if (x->ndim == 0){
@#ifdef _OPENMP
@#pragma omp single
@#endif /* _OPENMP */
{
        if (!where_flag) {
            if (!init_flag) {
                *py = *px;
            }
            else{
                @UNARY_OPERATOR@(*px,*py,bool)
            }
        } else {
            Bint *pw = (Bint *)w->ve_adr;
            if (*pw) {
                if (!init_flag) {
                    *py = *px;
                }
                else{
                    @UNARY_OPERATOR@(*px,*py,bool)
                }
            }
        }
} /* omp single */

/////////
// 1-d //
/////////
    } else if (x->ndim == 1){
        uint64_t i;
        uint64_t iw0 = 0;
        const uint64_t ix = x->strides[0]/x->itemsize;
        if (!where_flag) {
#if defined(add_reduce) || defined(multiply_reduce)
            int32_t tmp;
#if defined(add_reduce)
            nlcpy__global_bool = 0;
@#pragma omp barrier
            tmp = 0;
#elif defined(multiply_reduce)
            nlcpy__global_bool = 1;
@#pragma omp barrier
            tmp = 1;
#else
#error add_reduce or minimum_reduce must be defined.
#endif
            uint64_t ii;
            if (!init_flag) {
                *py = *px;
                ii = 1;
            } else {
                ii = 0;
            }
            const int it = omp_get_thread_num();
            const int nt = omp_get_max_threads();
            const uint64_t ist = (x->shape[0]-ii)*it/nt;
            const uint64_t ien = (x->shape[0]-ii)*(it+1)/nt;
            if (it==0) tmp = (int32_t)(*py);
@#pragma _NEC ivdep
            for (i=ii+ist; i < ii+ien; i++) {
                @UNARY_OPERATOR@(px[i*ix],tmp,bool)
            }
@#pragma _NEC novector
            for (i=0; i < nt; i++) {
                if (i==it) {
                    @UNARY_OPERATOR@(tmp,nlcpy__global_bool,bool)
                }
@#pragma omp barrier
            }
            *py = nlcpy__global_bool;
#elif ( defined(maximum_reduce) || defined(minimum_reduce) ) && ( defined(DTAG1_i32) || defined(DTAG1_i64) || defined(DTAG1_f32) || defined(DTAG1_f64) || defined(DTAG1_bool)) && ( "bool" eq "i32" || "bool" eq "i64" || "bool" eq "f32" || "bool" eq "f64" || "bool" eq "bool")
            // Note that rational operations for complex and unsigned numbers occurs an error or warnning.
            uint64_t ii;
            if (!init_flag) {
                *py = *px;
                ii = 1;
            } else {
                ii = 0;
            }
            nlcpy__global_bool = (int32_t)(*py);
@#pragma omp barrier
            const int it = omp_get_thread_num();
            const int nt = omp_get_max_threads();
            const uint64_t ist = (x->shape[0]-ii)*it/nt;
            const uint64_t ien = (x->shape[0]-ii)*(it+1)/nt;
            int32_t tmp = (int32_t)(*py);
#if "bool" eq "f32" || "bool" eq "f64"
            double is_there_nan = (isnan_bool(tmp)) ? 1.0 : 0.0;
#elif "bool" eq "i32" || "bool" eq "i64" || "bool" eq "bool"
            double is_there_nan = 0.0;
#else
#error Not Impletended
#endif
@#pragma _NEC ivdep
            for (i=ii+ist; i < ii+ien; i++) {
#if defined(maximum_reduce)
                tmp = (tmp > px[i*ix]) ? tmp : (int32_t)px[i*ix];
#elif defined(minimum_reduce)
                tmp = (tmp < px[i*ix]) ? tmp : (int32_t)px[i*ix];
#endif
#if defined(DTAG1_f32) || defined(DTAG1_f64)
                // NaN check:
                // is_there_nan += (float)isnan_@DTAG1@(px[i*ix]);
                @TYPE1@ xx;
                *(&xx) = px[i*ix];
                // The following line checks for NaN and it is in an unnatural way.
                // Here, NaN == NaN becomes False.
                // If there is one or more qNaN, signaling NaN (sNaN) occurs.
                // qNaN might not be detected qNaN by compiler optimizations.
                // However, we prioritize the performance.
                is_there_nan += (! (xx == px[i*ix]) ) ? 1.0 : 0.0;
#endif
            }
#if defined(DTAG1_f32) || defined(DTAG1_f64)
            if (is_there_nan != (float)0) {
                tmp = NAN;
                // In the following function call, PSW flags are manipulated to skip the intentional sNaN above.
                retrieve_fpe_flags(psw);
                *psw &= 0x0000003d;
                set_fpe_flags(*psw);
            }
#endif
@#pragma omp critical
            {
                @UNARY_OPERATOR@(tmp,nlcpy__global_bool,bool)
            }
@#pragma omp barrier
            *py = nlcpy__global_bool;
#else
        // Unvectorizable case
@#ifdef _OPENMP
@#pragma omp single
@#endif /* _OPENMP */
            {
                if (!init_flag) {
                    *py = *px;
                    i = 1;
                } else {
                    i = 0;
                }
@#pragma _NEC novector
                for (; i < x->shape[0]; i++) {
                    @UNARY_OPERATOR@(px[i*ix],*py,bool)
                }
            } /* omp single */
#endif
        } else {
@#ifdef _OPENMP
@#pragma omp single
@#endif /* _OPENMP */
            {
                Bint *pw = (Bint *)w->ve_adr;
                const uint64_t iw = w->strides[0]/w->itemsize;
                if (!init_flag) {
                    for (i = 0; i < x->shape[0]; i++) {
                        if (pw[i*iw]) {
                            *py = *px;
                            i++;
                            break;
                        }
                    }
                } else {
                    i = 0;
                }
#ifdef left_shift_reduce
@#pragma _NEC novector
#else
@#pragma _NEC ivdep
#endif
                for (; i < x->shape[0]; i++) {
                    if (pw[i*iw]) {
                        @UNARY_OPERATOR@(px[i*ix],*py,bool)
                    }
                }
            } /* omp single */
        }

/////////
// N-d //
/////////
    } else if (x->ndim <= NLCPY_MAXNDIM) {
        int64_t n_inner = x->ndim - 1;
        int64_t n_outer = 0;

        int64_t *idx = (int64_t*)alloca(sizeof(int64_t)*x->ndim);
        for (uint64_t i = 0; i < x->ndim; i++) {
            idx[i] = i;
        }
        if (x->ndim > 2) {
            int64_t max_idx[3], tmp;
            nlcpy__argnsort(x, max_idx, 3);
#ifndef add_reduce
            if (idx[max_idx[0]] == axis) {
                max_idx[0] = max_idx[1];
            }
#endif
            if (max_idx[0] != n_inner) {
                tmp = idx[n_inner];
                idx[n_inner] = idx[max_idx[0]];
                idx[max_idx[0]] = tmp;
            }
            if (idx[max_idx[1]] == axis) {
                max_idx[1] = max_idx[2];
            }
            if (idx[max_idx[1]] != n_outer) {
                tmp = idx[n_outer];
                idx[n_outer] = idx[max_idx[1]];
                idx[max_idx[1]] = tmp;
            }
        } else if (n_outer == axis) {
            idx[0] = 1;
            idx[1] = 0;
        }
        int64_t n_inner2 = idx[n_inner];
        int64_t n_outer2 = idx[n_outer];

@#ifdef _OPENMP
        const int nt = omp_get_num_threads();
        const int it = omp_get_thread_num();
@#else
        const int nt = 1;
        const int it = 0;
@#endif
        int64_t i;
        uint64_t adr_x = (uint64_t)x->ve_adr;
        uint64_t adr_y = (uint64_t)y->ve_adr;

        uint64_t ix = 0;
        uint64_t iy = 0;
        uint64_t iw = 0;
        uint64_t ix0 = x->strides[n_inner2] / x->itemsize;
        uint64_t iy0 = y->strides[n_inner2] / y->itemsize;
        uint64_t iw0 = 0;
        int64_t k = 0;
        int64_t *cnt_x = (int64_t*)alloca(sizeof(int64_t)*x->ndim);
        nlcpy__reset_coords(cnt_x, x->ndim);
        const int64_t lenm = x->shape[n_outer2];
        const int64_t cntm_s = lenm * it / nt;
        const int64_t cntm_e = lenm * (it + 1) / nt;
        if (!where_flag) {
            for (int64_t cntm = cntm_s; cntm < cntm_e; cntm++) {
                cnt_x[n_outer2] = cntm;
                ix = cntm * x->strides[n_outer2] / x->itemsize;
                iy = (n_outer2 == axis) ? 0 : cntm * y->strides[n_outer2] / y->itemsize;

                do {
                    if (n_inner2 == axis) {
                        // Note that a rational operation for complex and unsigned numbers occurs an error or warnning.
                        uint64_t ii;
                        if (!init_flag) {
                           py[iy] = px[ix];
                           ii = 1;
                        } else {
                           ii = 0;
                        }
#if ( defined(maximum_reduce) || defined(minimum_reduce) ) && ( defined(DTAG1_i32) || defined(DTAG1_i64) || defined(DTAG1_f32) || defined(DTAG1_f64) || defined(DTAG1_bool)) && ( "bool" eq "i32" || "bool" eq "i64" || "bool" eq "f32" || "bool" eq "f64" || "bool" eq "bool")
                        int32_t tmp = py[iy];
#if "bool" eq "f32" || "bool" eq "f64"
                        float is_there_nan = (isnan_bool(py[iy])) ? 1.0 : 0.0;
#elif "bool" eq "i32" || "bool" eq "i64" || "bool" eq "bool"
                        float is_there_nan = 0.0;
#else
#error Not Impletended
#endif
@#pragma _NEC ivdep
                        for (i = ii; i < x->shape[n_inner2]; i++) {
#if defined(maximum_reduce)
                            tmp = (tmp > px[i*ix0+ix]) ? tmp : (int32_t)px[i*ix0+ix];
#elif defined(minimum_reduce)
                            tmp = (tmp < px[i*ix0+ix]) ? tmp : (int32_t)px[i*ix0+ix];
#endif
#if defined(DTAG1_f32) || defined(DTAG1_f64)
                            // NaN check:
                            //   is_there_nan += (float)isnan_@DTAG1@(px[i*ix0+ix]);
                            @TYPE1@ xx;
                            *(&xx) = px[i*ix0+ix];
                            // The following line checks for NaN and it is in an unnatural way.
                            // Here, NaN == NaN becomes False.
                            // If there is one or more qNaN, signaling NaN (sNaN) occurs.
                            // qNaN might not be detected qNaN by compiler optimizations.
                            // However, we prioritize the performance.
                            is_there_nan += (! (xx == px[i*ix0+ix]) ) ? 1.0 : 0.0;
#endif
                        }
#if defined(DTAG1_f32) || defined(DTAG1_f64)
                        if (is_there_nan != (float)0) {
                            tmp = NAN;
                            // In the following function call, PSW flags are manipulated to skip the intentional sNaN above.
                            retrieve_fpe_flags(psw);
                            *psw &= 0x0000003d;
                            set_fpe_flags(*psw);
                        }
#endif
                        py[iy] = tmp;
#else
                        // General case
#ifdef left_shift_reduce
@#pragma _NEC novector
#else
@#pragma _NEC ivdep
#endif
                        for (i = ii; i < x->shape[n_inner2]; i++) {
                            @UNARY_OPERATOR@(px[i*ix0+ix],py[iy],bool)
                        }
#endif
                    } else {
                        if (init_flag || cnt_x[axis] > 0) {
#ifdef left_shift_reduce
@#pragma _NEC novector
#else
@#pragma _NEC ivdep
#endif
                            for (i = 0; i < x->shape[n_inner2]; i++) {
                                @UNARY_OPERATOR@(px[i*ix0+ix],py[i*iy0+iy],bool)
                             }
                        } else {
                            for (i = 0; i < x->shape[n_inner2]; i++) {
                                py[i*iy0+iy] = @CAST_OPERATOR@(px[i*ix0+ix],@DTAG1@,bool)
                            }
                        }
                    }
                    for (k = n_inner - 1; k >= 1; k--) {
                        int64_t kk = idx[k];
                        if (kk == axis) {
                            if (++cnt_x[kk] < x->shape[kk]) {
                                ix += x->strides[kk] / x->itemsize;
                                break;
                            }
                            ix -= (x->strides[kk] / x->itemsize) * (x->shape[kk] - 1);
                        } else {
                            if (++cnt_x[kk] < x->shape[kk]) {
                                ix += x->strides[kk] / x->itemsize;
                                iy += y->strides[kk] / y->itemsize;
                                break;
                            }
                            ix -= (x->strides[kk] / x->itemsize) * (x->shape[kk] - 1);
                            iy -= (y->strides[kk] / y->itemsize) * (y->shape[kk] - 1);
                        }
                        cnt_x[kk] = 0;
                    }
                } while (k >= 1);
            }
        } else {
            Bint *pw = (Bint *)w->ve_adr;
            iw0 = w->strides[n_inner2] / w->itemsize;

            for (int64_t cntm = cntm_s; cntm < cntm_e; cntm++) {
                ix = cntm * x->strides[n_outer2] / x->itemsize;
                iy = (n_outer2 == axis) ? 0 : cntm * y->strides[n_outer2] / y->itemsize;
                iw = cntm * w->strides[n_outer2] / w->itemsize;

                do {
                    if (n_inner2 == axis) {
                        if (!init_flag) {
                            for (i = 0; i < w->shape[n_inner2]; i++) {
                                if (pw[i*iw0+iw]) {
                                    py[iy] = @CAST_OPERATOR@(px[i*ix0+ix],@DTAG1@,bool)
                                    i++;
                                    break;
                                }
                            }
                        } else {
                            i = 0;
                        }
#ifdef left_shift_reduce
@#pragma _NEC novector
#else
@#pragma _NEC ivdep
#endif
                        for (; i < x->shape[n_inner2]; i++) {
                            if (pw[i*iw0+iw]) {
                                @UNARY_OPERATOR@(px[i*ix0+ix],py[iy],bool)
                            }
                        }
                    } else {
                        if (init_flag || cnt_x[axis] > 0) {
#ifdef left_shift_reduce
@#pragma _NEC novector
#else
@#pragma _NEC ivdep
#endif
                            for (i = 0; i < x->shape[n_inner2]; i++) {
                                if (pw[i*iw0+iw]) {
                                    @UNARY_OPERATOR@(px[i*ix0+ix],py[i*iy0+iy],bool)
                                }
                            }
                        } else {
                            for (i = 0; i < x->shape[n_inner2]; i++) {
                                if (pw[i*iw0+iw]) {
                                    py[i*iy0+iy] = @CAST_OPERATOR@(px[i*ix0+ix],@DTAG1@,bool)
                                }
                            }
                        }
                    }
                    for (k = n_inner - 1; k >= 1; k--) {
                        int64_t kk = idx[k];
                        if (kk == axis) {
                            if (++cnt_x[kk] < x->shape[kk]) {
                                ix += x->strides[kk] / x->itemsize;
                                iw += w->strides[kk] / w->itemsize;
                                break;
                            }
                            ix -= (x->strides[kk] / x->itemsize) * (x->shape[kk] - 1);
                            iw -= (w->strides[kk] / w->itemsize) * (x->shape[kk] - 1);
                        } else {
                            if (++cnt_x[kk] < x->shape[kk]) {
                                ix += x->strides[kk] / x->itemsize;
                                iy += y->strides[kk] / y->itemsize;
                                iw += w->strides[kk] / w->itemsize;
                                break;
                            }
                            ix -= (x->strides[kk] / x->itemsize) * (x->shape[kk] - 1);
                            iy -= (y->strides[kk] / y->itemsize) * (x->shape[kk] - 1);
                            iw -= (w->strides[kk] / w->itemsize) * (x->shape[kk] - 1);
                        }
                        cnt_x[kk] = 0;
                    }
                } while (k >= 1);
            }
        }
    } else {
        return (uint64_t)NLCPY_ERROR_NDIM;
    }
    retrieve_fpe_flags(psw);
    return (uint64_t)NLCPY_ERROR_OK;
}


#endif
